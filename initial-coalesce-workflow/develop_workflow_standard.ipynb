{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username='harrygibson'\n",
    "password=getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=r\"C:\\Users\\zool1301\\Documents\\GitHub\\modis_acquisition_pipeline\\dev\\map-oxford-hsg-bf2341e4c187.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_FROM = 2000\n",
    "YEAR_TO = 2020\n",
    "DOY_START = 1\n",
    "DOY_END = -1\n",
    "TILE = '*'\n",
    "BASE_URL = \"http://e4ftl01.cr.usgs.gov\"\n",
    "platform = \"MOLT\"\n",
    "product = \"MOD11A2.006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_url = f\"{BASE_URL}/{platform}/{product}\"\n",
    "product_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_selected_dates(year_from=2000, year_to=2020, doy_start=1, doy_end=-1):\n",
    "    import calendar, time\n",
    "    dates = []\n",
    "    for year in range(year_from, year_to+1):\n",
    "        if doy_end == -1:\n",
    "            if calendar.isleap(year):\n",
    "                end_day = 367\n",
    "            else:\n",
    "                end_day = 366\n",
    "        else:\n",
    "            end_day = doy_end\n",
    "        dates_this_yr = [time.strftime(\"%Y.%m.%d\", time.strptime(\"%d/%d\" % (i, year),\n",
    "                                                         \"%j/%Y\")) for i in\n",
    "                 range(doy_start, end_day)]\n",
    "        dates.extend(dates_this_yr)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_existing_files(out_dir):\n",
    "    # in case we need to do something different to list files on bucket\n",
    "    return os.listdir(out_dir)\n",
    "\n",
    "def load_page_text(url):\n",
    "    import requests\n",
    "    # nasa data pools are unavailable for maintenance on wednesday afternoons\n",
    "    the_day_today = time.asctime().split()[0]\n",
    "    the_hour_now = int(time.asctime().split()[3].split(\":\")[0])\n",
    "    if the_day_today == \"Wed\" and 14 <= the_hour_now <= 17:\n",
    "        LOG.info(\"Sleeping for %d hours... Yawn!\" % (18 - the_hour_now))\n",
    "        time.sleep(60 * 60 * (18 - the_hour_now))\n",
    "    resp = requests.get(url)\n",
    "    return resp.text\n",
    "    \n",
    "def parse_modis_dates (product_url, requested_dates, product, out_dir, check_existing_dates=False ):\n",
    "    \"\"\"Parse returned MODIS dates.\n",
    "\n",
    "    This function gets the dates listing for a given MODIS products, and\n",
    "    extracts the dates for when data is available. Further, it crosses these\n",
    "    dates with the required dates that the user has selected and returns the\n",
    "    intersection. Additionally, if the `checkExistingDates` flag is set, we'll check for\n",
    "    files that might already be present in the system and skip them. Note\n",
    "    that if a file failed in downloading, it might still be around\n",
    "    incomplete.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        A top level product URL such as \"http://e4ftl01.cr.usgs.gov/MOTA/MCD45A1.005/\"\n",
    "    dates: list\n",
    "        A list of required dates in the format \"YYYY.MM.DD\"\n",
    "    product: str\n",
    "        The product name, MOD09GA.005\n",
    "    out_dir: str\n",
    "        The output dir\n",
    "    checkExistingDates: bool\n",
    "        Whether to check for present files\n",
    "    Returns\n",
    "    -------\n",
    "    A (sorted) list with the dates that will be downloaded.\n",
    "    \"\"\"\n",
    "    if check_existing_dates:\n",
    "        product = product_url.strip('/').split('/')[-1]\n",
    "        product_no_version = product.split(\".\")[0]\n",
    "        already_here = fnmatch.filter(get_existing_files(out_dir),\n",
    "                                      \"%s*hdf\" % product_no_version)\n",
    "        already_here_dates = [x.split(\".\")[-5][1:]\n",
    "                              for x in already_here]\n",
    "\n",
    "    html = load_page_text(product_url)\n",
    "\n",
    "    available_dates = []\n",
    "    for line in html:\n",
    "        if line.find(\"href\") >= 0 and \\\n",
    "                        line.find(\"[DIR]\") >= 0:\n",
    "            # Points to a directory\n",
    "            the_date = line.split('href=\"')[1].split('\"')[0].strip(\"/\")\n",
    "            if check_existing_dates:\n",
    "                try:\n",
    "                    modis_date = time.strftime(\"%Y%j\",\n",
    "                                               time.strptime(the_date,\n",
    "                                                             \"%Y.%m.%d\"))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if modis_date in already_here_dates:\n",
    "                    continue\n",
    "                else:\n",
    "                    available_dates.append(the_date)\n",
    "            else:\n",
    "                available_dates.append(the_date)\n",
    "\n",
    "    dates = set(dates)\n",
    "    available_dates = set(available_dates)\n",
    "    suitable_dates = list(dates.intersection(available_dates))\n",
    "    suitable_dates.sort()\n",
    "    return suitable_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dates = generate_selected_dates(YEAR_FROM, YEAR_TO, DOY_START, DOY_END)\n",
    "dates_to_download = parse_modis_dates(product_url, real_dates, product, \"C:\\\\temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will make downloadable_dates the point at which we split off jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_download_urls_for_date(product_url, date, tile='*'):\n",
    "    date_page_url = product_url + \"/\" + date\n",
    "    print(date_page_url)\n",
    "    date_page_html = load_page_text(date_page_url)\n",
    "    hdf_urls = []\n",
    "    hdf_lines = [i for i in [line for line in date_page_html.split('\\n') \\\n",
    "                    if (tile == '*' or # download all tiles\n",
    "                        (isinstance(tile, list) and any(t in line for t in tile)) or # a list of tiles\n",
    "                        (line.find(tile) != -1))] # a single tile\n",
    "                 if i.find('.hdf\"') != -1]\n",
    "    hdf_files = [date_page_url + '/' + l.split('<a href=\"')[1].split('\">')[0] for l in hdf_lines]\n",
    "    return hdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2000.03.05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v03.006.2015057173849.hdf',\n",
       " 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v04.006.2015057173846.hdf',\n",
       " 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v03.006.2015057173926.hdf',\n",
       " 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v04.006.2015057173924.hdf']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_date = dates_to_download[2]\n",
    "this_job_download_urls = get_download_urls_for_date(product_url, dates_to_download[2], ['h17v03', 'h18v03', 'h17v04', 'h18v04'])\n",
    "this_job_download_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hdfs_to_bucket(hdf_urls, dest_folder, nasa_username, nasa_pw):\n",
    "    import requests, tempfile, os\n",
    "    from google.cloud import storage\n",
    "    storage_client = storage.Client(\"map-oxford-hsg\")\n",
    "    # Create a bucket object for our bucket\n",
    "    bucket = storage_client.get_bucket(\"hsg-dataflow-test\")\n",
    "    with requests.Session() as s:\n",
    "        s.auth = (nasa_username, nasa_pw)\n",
    "        for url in hdf_urls:\n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    r1 = s.request('get', url)\n",
    "                    r = s.get(r1.url, stream=True)\n",
    "                    if not r.ok:\n",
    "                        raise IOError(\"Can't start download for {}\".format(url))\n",
    "                    product, datestr, fname = url.split(\"/\")[-3:]\n",
    "                    tempfilename = os.path.join(tempfile.gettempdir(), fname)\n",
    "                    with open(tempfilename, 'wb') as fp:\n",
    "                        for chunk in r.iter_content(chunk_size=65536):\n",
    "                            if chunk:\n",
    "                                fp.write(chunk)\n",
    "                        fp.flush()\n",
    "                        os.fsync(fp)\n",
    "                    # Create a blob object from the filepath\n",
    "                    print(f\"Downloaded {fname}\")\n",
    "                    bucketpath = '/'.join([dest_folder, product, datestr, fname])\n",
    "                    blob = bucket.blob(bucketpath)\n",
    "                    blob.upload_from_filename(tempfilename)\n",
    "                    print(f\"Uploaded {fname} to bucket\")\n",
    "                    os.remove(tempfilename)\n",
    "                except:\n",
    "                    raise\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                raise IOError(\"conneection error occurred 10 times on \"+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded MOD11A2.A2000065.h17v03.006.2015057173849.hdf\n",
      "Uploaded MOD11A2.A2000065.h17v03.006.2015057173849.hdf to bucket\n",
      "Downloaded MOD11A2.A2000065.h17v04.006.2015057173846.hdf\n",
      "Uploaded MOD11A2.A2000065.h17v04.006.2015057173846.hdf to bucket\n",
      "Downloaded MOD11A2.A2000065.h18v03.006.2015057173926.hdf\n",
      "Uploaded MOD11A2.A2000065.h18v03.006.2015057173926.hdf to bucket\n",
      "Downloaded MOD11A2.A2000065.h18v04.006.2015057173924.hdf\n",
      "Uploaded MOD11A2.A2000065.h18v04.006.2015057173924.hdf to bucket\n"
     ]
    }
   ],
   "source": [
    "download_hdfs_to_bucket(this_job_download_urls, \"dev_hdf\", username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v03.006.2015057173849.hdf',\n",
       " 'dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v04.006.2015057173846.hdf',\n",
       " 'dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v03.006.2015057173926.hdf',\n",
       " 'dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v04.006.2015057173924.hdf']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_day_blobs = list(bucket.list_blobs(prefix=f\"dev_hdf/{product}/{test_date}\"))\n",
    "[b.name for b in this_day_blobs if b.name.split('/')[2] == test_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v03.006.2015057173849.hdf',\n",
       " 'gs://hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v04.006.2015057173846.hdf',\n",
       " 'gs://hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v03.006.2015057173926.hdf',\n",
       " 'gs://hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v04.006.2015057173924.hdf']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_template_lst_file = \"gs://hsg-dataflow-test/{}\" \n",
    "gs_paths = [gs_template_lst_file.format(blob.name) for blob in this_day_blobs]\n",
    "gs_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v03.006.2015057173849.hdf...\n",
      "/ [0 files][    0.0 B/  3.1 MiB]                                                \n",
      "-\n",
      "- [1 files][  3.1 MiB/  3.1 MiB]                                                \n",
      "\n",
      "Operation completed over 1 objects/3.1 MiB.                                      \n",
      "Copying gs://hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v04.006.2015057173846.hdf...\n",
      "/ [0 files][    0.0 B/  3.4 MiB]                                                \n",
      "-\n",
      "- [1 files][  3.4 MiB/  3.4 MiB]                                                \n",
      "\n",
      "Operation completed over 1 objects/3.4 MiB.                                      \n",
      "Copying gs://hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v03.006.2015057173926.hdf...\n",
      "/ [0 files][    0.0 B/  5.2 MiB]                                                \n",
      "-\n",
      "- [0 files][  2.8 MiB/  5.2 MiB]                                                \n",
      "\\\n",
      "\\ [1 files][  5.2 MiB/  5.2 MiB]                                                \n",
      "\n",
      "Operation completed over 1 objects/5.2 MiB.                                      \n",
      "Copying gs://hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v04.006.2015057173924.hdf...\n",
      "/ [0 files][    0.0 B/  6.6 MiB]                                                \n",
      "-\n",
      "- [0 files][  5.9 MiB/  6.6 MiB]                                                \n",
      "- [1 files][  6.6 MiB/  6.6 MiB]                                                \n",
      "\n",
      "Operation completed over 1 objects/6.6 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "tempfolder = tempfile.gettempdir()\n",
    "tmppaths = []\n",
    "for f in gs_paths:\n",
    "    tmppath = os.path.join(tempfolder, os.path.basename(f))\n",
    "    !gsutil cp {f} {tmppath}\n",
    "    tmppaths.append(tmppath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gdal.BuildVRT(os.path.join(tempfolder, \"test.vrt\"), tmppaths)\n",
    "d.FlushCache()\n",
    "d=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_template_lst_day_lyr = \"gs://hsg-dataflow-test/{}:MODIS_Grid_8Day_1km_LST:LST_Day_1km\\n\"\n",
    "gs_template_lst_night_lyr = \"gs://hsg-dataflow-test/{}:MODIS_Grid_8Day_1km_LST:LST_Night_1km\\n\"\n",
    "vrtListDay = [lst_day_template.format(blob.name) for blob in this_day_blobs]\n",
    "vrtListNight = [lst_night_template.format(blob.name) for blob in this_day_blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vrtListDayFile = os.path.join(tempfile.gettempdir(), \"LST_Day_Files_\"+test_date+\".txt\")\n",
    "vrtDayFile = vrtListDayFile.replace('.txt', '.vrt')   \n",
    "\n",
    "vrtListDayFileLocal = os.path.join(tempfile.gettempdir(), \"LST_Day_Files_Local\"+test_date+\".txt\")\n",
    "vrtDayFileLocal = vrtListDayFileLocal.replace('.txt', '.vrt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vrtListDayFile, 'w') as txtfile:\n",
    "    txtfile.writelines(vrtListDay)\n",
    "\n",
    "with open(vrtListDayFileLocal, 'w') as txtfile:\n",
    "    txtfile.writelines(vrtListDayLocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Blob: hsg-dataflow-test, dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v03.006.2015057173849.hdf, 1591365860649300>,\n",
       " <Blob: hsg-dataflow-test, dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v04.006.2015057173846.hdf, 1591365871783016>,\n",
       " <Blob: hsg-dataflow-test, dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v03.006.2015057173926.hdf, 1591365880871137>,\n",
       " <Blob: hsg-dataflow-test, dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v04.006.2015057173924.hdf, 1591365898026581>]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_day_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gdalbuildvrt -input_file_list C:\\\\Users\\\\zool1301\\\\AppData\\\\Local\\\\Temp\\\\LST_Day_Files_Local2000.03.05.txt C:\\\\Users\\\\zool1301\\\\AppData\\\\Local\\\\Temp\\\\LST_Day_Files_Local2000.03.05.vrt -te -20015109.356 -10007554.678 20015109.356 10007554.678 -tr 926.625433138760630 926.625433138788940'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrtCommandLocal = f\"gdalbuildvrt -input_file_list {vrtListDayFileLocal} {vrtDayFileLocal} -te -20015109.356 -10007554.678 20015109.356 10007554.678 -tr 926.625433138760630 926.625433138788940\"\n",
    "vrtCommandLocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 6: CPLRSASHA256Sign() not implemented: GDAL must be built against libcrypto++ or libcrypto (openssl)\n",
      "ERROR 4: `/vsigs_streaming/hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v03.006.2015057173849.hdf:MODIS_Grid_8Day_1km_LST:LST_Day_1km' does not exist in the file system, and is not recognized as a supported dataset name.\n",
      "Warning 1: Can't open /vsigs_streaming/hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v03.006.2015057173849.hdf:MODIS_Grid_8Day_1km_LST:LST_Day_1km. Skipping it\n",
      "ERROR 6: CPLRSASHA256Sign() not implemented: GDAL must be built against libcrypto++ or libcrypto (openssl)\n",
      "ERROR 4: `/vsigs_streaming/hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v04.006.2015057173846.hdf:MODIS_Grid_8Day_1km_LST:LST_Day_1km' does not exist in the file system, and is not recognized as a supported dataset name.\n",
      "Warning 1: Can't open /vsigs_streaming/hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v04.006.2015057173846.hdf:MODIS_Grid_8Day_1km_LST:LST_Day_1km. Skipping it\n",
      "ERROR 6: CPLRSASHA256Sign() not implemented: GDAL must be built against libcrypto++ or libcrypto (openssl)\n",
      "ERROR 4: `/vsigs_streaming/hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v03.006.2015057173926.hdf:MODIS_Grid_8Day_1km_LST:LST_Day_1km' does not exist in the file system, and is not recognized as a supported dataset name.\n",
      "Warning 1: Can't open /vsigs_streaming/hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v03.006.2015057173926.hdf:MODIS_Grid_8Day_1km_LST:LST_Day_1km. Skipping it\n",
      "ERROR 6: CPLRSASHA256Sign() not implemented: GDAL must be built against libcrypto++ or libcrypto (openssl)\n",
      "ERROR 4: `/vsigs_streaming/hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v04.006.2015057173924.hdf:MODIS_Grid_8Day_1km_LST:LST_Day_1km' does not exist in the file system, and is not recognized as a supported dataset name.\n",
      "Warning 1: Can't open /vsigs_streaming/hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h18v04.006.2015057173924.hdf:MODIS_Grid_8Day_1km_LST:LST_Day_1km. Skipping it\n"
     ]
    }
   ],
   "source": [
    "!{vrtCommandLocal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check that vrt creation works on dataflow, based on how gs is presented there\n",
    "\n",
    "Assuming it does, then calculate output tiffs in sinusoidal based on how the NDVI sample does it\n",
    "\n",
    "Then translate with gdalwarp as per my code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this does not work for hdf but ok for tif\n",
    "with beam.io.gcp.gcsio.GcsIO().open('gs://hsg-dataflow-test/dev_hdf/MOD11A2.006/2000.03.05/MOD11A2.A2000065.h17v03.006.2015057173849.hdf', 'rb') as f:\n",
    "    content = f.read()\n",
    "    gdal.FileFromMemBuffer('/vsimem/som_memfile', f.read())\n",
    "    ds = gdal.Open('/vsimem/som_memfile')\n",
    "    print(ds.GetGeoTransform())\n",
    "    ds = None\n",
    "    gdal.Unlink('/vsimem/som_memfile')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
