{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements\n",
    "\n",
    "The cloud dataflow VMs don't have gdal installed, and it's a bit of a nightmare to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CPLUS_INCLUDE_PATH']=\"/usr/include/gdal\"\n",
    "os.environ['C_INCLUDE_PATH']=\"/usr/include/gdal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get --assume-yes install gdal-bin libgdal-dev python3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "!gdal-config --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdal==2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/osgeo/gdal.py\n"
     ]
    }
   ],
   "source": [
    "!python -c \"from osgeo import gdal;print(gdal.__file__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/opt/conda/lib/python3.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.io.gcp.gcsio import GcsIO\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ·········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username='harrygibson'\n",
    "password=getpass()\n",
    "YEAR_FROM = 2019\n",
    "YEAR_TO = 2020\n",
    "DOY_START = 20\n",
    "DOY_END = 40\n",
    "TILE = '*'\n",
    "BASE_URL = \"http://e4ftl01.cr.usgs.gov\"\n",
    "platform = \"MOLT\"\n",
    "product = \"MOD11A2.006\"\n",
    "product_url = f\"{BASE_URL}/{platform}/{product}\"\n",
    "product_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_PATH = \"gs://hsg-dataflow-test/lst_download_dev\"\n",
    "HDF_BUCKET_PATH = BUCKET_PATH + '/hdf' #NB not os.path.join as that breaks on windows with backslash paths\n",
    "OUTPUT_BUCKET_PATH = BUCKET_PATH + \"/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to parse dates from the MODIS DAAC pages\n",
    "\n",
    "These are local functions, they don't need to run within the Beam pipeline, there's only a single http fetch and then some local list processing. They're more or less taken from get_modis (https://github.com/jgomezdans/get_modis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_selected_dates(year_from=2000, year_to=2020, doy_start=1, doy_end=-1):\n",
    "    import calendar, time\n",
    "    dates = []\n",
    "    for year in range(year_from, year_to+1):\n",
    "        if doy_end == -1:\n",
    "            if calendar.isleap(year):\n",
    "                end_day = 367\n",
    "            else:\n",
    "                end_day = 366\n",
    "        else:\n",
    "            end_day = doy_end\n",
    "        dates_this_yr = [time.strftime(\"%Y.%m.%d\", time.strptime(\"%d/%d\" % (i, year),\n",
    "                                                         \"%j/%Y\")) for i in\n",
    "                 range(doy_start, end_day)]\n",
    "        dates.extend(dates_this_yr)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(out_dir):\n",
    "    # in case we need to do something different to list files on bucket\n",
    "    return os.listdir(out_dir)\n",
    "\n",
    "def load_page_text(url):\n",
    "    import requests, time\n",
    "    # nasa data pools are unavailable for maintenance on wednesday afternoons\n",
    "    the_day_today = time.asctime().split()[0]\n",
    "    the_hour_now = int(time.asctime().split()[3].split(\":\")[0])\n",
    "    if the_day_today == \"Wed\" and 14 <= the_hour_now <= 17:\n",
    "        LOG.info(\"Sleeping for %d hours... Yawn!\" % (18 - the_hour_now))\n",
    "        time.sleep(60 * 60 * (18 - the_hour_now))\n",
    "    resp = requests.get(url)\n",
    "    return resp.text\n",
    "    \n",
    "def parse_modis_dates (product_url, requested_dates, product, out_dir, check_existing_dates=False ):\n",
    "    \"\"\"Parse returned MODIS dates.\n",
    "\n",
    "    This function gets the dates listing for a given MODIS products, and\n",
    "    extracts the dates for when data is available. Further, it crosses these\n",
    "    dates with the required dates that the user has selected and returns the\n",
    "    intersection. Additionally, if the `checkExistingDates` flag is set, we'll check for\n",
    "    files that might already be present in the system and skip them. Note\n",
    "    that if a file failed in downloading, it might still be around\n",
    "    incomplete.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        A top level product URL such as \"http://e4ftl01.cr.usgs.gov/MOTA/MCD45A1.005/\"\n",
    "    dates: list\n",
    "        A list of required dates in the format \"YYYY.MM.DD\"\n",
    "    product: str\n",
    "        The product name, MOD09GA.005\n",
    "    out_dir: str\n",
    "        The output dir\n",
    "    checkExistingDates: bool\n",
    "        Whether to check for present files\n",
    "    Returns\n",
    "    -------\n",
    "    A (sorted) list with the dates that will be downloaded.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    if check_existing_dates:\n",
    "        product = product_url.strip('/').split('/')[-1]\n",
    "        product_no_version = product.split(\".\")[0]\n",
    "        already_here = fnmatch.filter(get_existing_files(out_dir),\n",
    "                                      \"%s*hdf\" % product_no_version)\n",
    "        already_here_dates = [x.split(\".\")[-5][1:]\n",
    "                              for x in already_here]\n",
    "\n",
    "    html = load_page_text(product_url).split('\\n')\n",
    "\n",
    "    available_dates = []\n",
    "    for line in html:\n",
    "        if line.find(\"href\") >= 0 and \\\n",
    "                        line.find(\"[DIR]\") >= 0:\n",
    "            # Points to a directory\n",
    "            the_date = line.split('href=\"')[1].split('\"')[0].strip(\"/\")\n",
    "            if check_existing_dates:\n",
    "                try:\n",
    "                    modis_date = time.strftime(\"%Y%j\",\n",
    "                                               time.strptime(the_date,\n",
    "                                                             \"%Y.%m.%d\"))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if modis_date in already_here_dates:\n",
    "                    continue\n",
    "                else:\n",
    "                    available_dates.append(the_date)\n",
    "            else:\n",
    "                available_dates.append(the_date)\n",
    "\n",
    "    dates = set(requested_dates)\n",
    "    available_dates = set(available_dates)\n",
    "    suitable_dates = list(dates.intersection(available_dates))\n",
    "    suitable_dates.sort()\n",
    "    return suitable_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a list of the dates for which data are required and available\n",
    "\n",
    "This will be the input to our pipeline (possibly along with tiles). We will set up three test cases:\n",
    "* four arbitrary dates\n",
    "* all dates in one year (2019)\n",
    "* all dates available\n",
    "\n",
    "In each case we will generate a list of the string dates then make a pipeline to build them into a PCollection comprising the URLs of the page for the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dates = generate_selected_dates(YEAR_FROM, YEAR_TO, DOY_START, DOY_END)\n",
    "TEST_DATES_SPECIFIC = parse_modis_dates(product_url, real_dates, product, \"C:\\\\temp\")\n",
    "\n",
    "all_2019_dates = generate_selected_dates(2019, 2019, 1, 366)\n",
    "TEST_DATES_WHOLE_YEAR = parse_modis_dates(product_url, all_2019_dates, product, False)\n",
    "\n",
    "all_available_dates = generate_selected_dates(2000, 2020, 1, 366)\n",
    "TEST_DATES_ALL_TIME = parse_modis_dates(product_url, all_available_dates, product, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set up three test cases for which tiles should be downloaded:\n",
    "\n",
    "* A block of four tiles - we'll test this for all available dates\n",
    "* All tiles in Africa - we'll test this for a whole year\n",
    "* All tiles globally - we'll test this for four dates only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TILES_FOUR = ['h17v03', 'h18v03', 'h17v04', 'h18v04']\n",
    "\n",
    "#africa: h16_23 v5_12\n",
    "import itertools\n",
    "TEST_TILES_AFRICA = [f'h{pair[0]:02}v{pair[1]:02}' for pair in (itertools.product(range(16,24), range(5,13)))]\n",
    "\n",
    "TEST_TILES_ALL = '*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which one we're doing\n",
    "\n",
    "MODIFY THIS NEXT CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATES_TO_TEST = [TEST_DATES_SPECIFIC, TEST_DATES_WHOLE_YEAR, TEST_DATES_ALL_TIME][0]\n",
    "TILES_TO_TEST = [TEST_TILES_FOUR, TEST_TILES_AFRICA, TEST_TILES_ALL][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the HDF urls from the date pages\n",
    "\n",
    "For each date, we need to load the page and parse the URLs to the HDF files. Define a PTransform to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDatePageUrl(beam.DoFn):\n",
    "    def __init__(self, producturl):\n",
    "        self.producturl = producturl\n",
    "    def process(self, date):\n",
    "        return [self.producturl + \"/\" + date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetHdfUrlsForDate(beam.PTransform):\n",
    "                \n",
    "    def load_page_text_to_lines(self, url):\n",
    "        import requests\n",
    "        resp = requests.get(url)\n",
    "        lines = resp.text.split('\\n')\n",
    "        return [(l, url) for l in lines]\n",
    "        #return beam.Create(lines)\n",
    "    \n",
    "    def parse_hdf_from_line(self, textline, baseurl):\n",
    "        if textline.find('.hdf\"') != -1:\n",
    "            return baseurl + '/' + textline.split('<a href=\"')[1].split('\">')[0]\n",
    "        \n",
    "    def expand(self, pcoll):\n",
    "        return (pcoll\n",
    "                | \"Load_page_lines\" >> beam.FlatMap(self.load_page_text_to_lines)\n",
    "                | \"discover_hdf_urls\" >> beam.MapTuple(lambda line, url:self.parse_hdf_from_line(line, url))\n",
    "                | \"remove_non_matching\" >> beam.Filter(lambda l: l is not None)\n",
    "               )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to only the required HDF URLs\n",
    "\n",
    "Only keep those which are for a required tile and which we have not already downloaded to the given bucket path.\n",
    "\n",
    "Ideally this should be developed to take account of the version part of their filename.\n",
    "\n",
    "Define a PTransform for this which takes the bucket path and required tiles as side inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_existing_files(beam.PTransform):\n",
    "    def __init__(self, hdf_bucket_path, req_tile_list=\"*\"):\n",
    "        gcs = GcsIO()\n",
    "        self._existing = [os.path.basename(l) for l in list(gcs.list_prefix(hdf_bucket_path).keys())]\n",
    "        self._required_tiles = req_tile_list\n",
    "        \n",
    "    def checktile(self, url):\n",
    "        import os\n",
    "        thistile = os.path.basename(url).split('.')[2]\n",
    "        return self._required_tiles == \"*\" or thistile in self._required_tiles\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        import os\n",
    "        stripped_existing = (pcoll | \"remove_existing\" >> beam.Filter(lambda l: os.path.basename(l) \n",
    "                                                        not in self._existing))\n",
    "        if self._required_tiles == \"*\":\n",
    "            return stripped_existing\n",
    "        else:\n",
    "            return stripped_existing | \"remove_unrequired\" >> beam.Filter(lambda l: self.checktile(l))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the files\n",
    "\n",
    "Now we can actually download those files. Define a PTransform which does this. We don't return the actual downloaded data in the pipeline, we save it to the bucket as a \"side effect\" and return the bucket path.\n",
    "\n",
    "The PTransform class maintains an authenticated session object as a member variable as i guess this might be better than logging in for every tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadHdfsToBucket(beam.PTransform):\n",
    "   \n",
    "    def __init__(self, user, pw, hdf_bucket_path):\n",
    "        import requests\n",
    "        self._session = requests.Session()\n",
    "        self._session.auth = (user, pw)\n",
    "        self._hdf_bucket_path = hdf_bucket_path\n",
    "        self._chunk_size = 8 * 1024 * 1024\n",
    "    \n",
    "    def download_file(self, url):\n",
    "        import requests, tempfile, os\n",
    "        from apache_beam.io.gcp.gcsio import GcsIO\n",
    "        req = self._session.request('get', url)\n",
    "        resp = self._session.get(req.url, stream=True)\n",
    "        product, datestr, fname = url.split('/')[-3:]\n",
    "        bucketfilename = '/'.join([self._hdf_bucket_path, product, datestr, fname])\n",
    "        gcs = GcsIO()\n",
    "        with gcs.open(bucketfilename, 'w') as fp:\n",
    "        #with open(tempfilename, 'wb') as fp:\n",
    "            for chunk in resp.iter_content(chunk_size=self._chunk_size):\n",
    "                if chunk:\n",
    "                    fp.write(chunk)\n",
    "            fp.flush()\n",
    "            #os.fsync(fp)\n",
    "        return bucketfilename\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return(pcoll | beam.Map(self.download_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Virtually) mosaic the files\n",
    "\n",
    "Build a GDAL vrt for each day's files to mosaic them together. This requires knowledge of which layer needs to be extracted from the HDF. We will have to modify this (or rather, pass in a string template for formatting) to make it more flexible.\n",
    "\n",
    "This transform needs to be called once for every day (not once for every HDF). At the minute we hack the date back out of the bucket path. It might be better to modify the download function to return tuples of (date, list_of_tiles) then it can just pass through into this in a single pipeline.\n",
    "\n",
    "Although GDAL can in theory read inputs straight from bucket storage (/vsigs/) this isn't necessarily built in and needs a different authentication flow so would be a lot more complex. Instead we copy the files back to worker's local storage and work on them there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateVrtsForDays(beam.PTransform):\n",
    "    def __init__(self, bucketpath):\n",
    "        import os\n",
    "        from apache_beam.io.gcp.gcsio import GcsIO\n",
    "        self._hdfpath = bucketpath\n",
    "        gcs = GcsIO()\n",
    "        self._existing = [l for l in list(gcs.list_prefix(hdf_bucket_path).keys())]\n",
    "            \n",
    "    def get_tilenames_for_day(self, day):\n",
    "        return [f for f in self._existing if f.split('/')[-2] == day]\n",
    "    \n",
    "    def get_tmp_folder_for_day(self, day):\n",
    "        tmpfolder = tempfile.gettempdir()\n",
    "        workfolder = os.path.join(tmpfolder, day)\n",
    "        return workfolder\n",
    "    \n",
    "    def localise_day_files(self, day):\n",
    "        files = self.get_tilenames_for_day(day)\n",
    "        tempfolder = self.get_tmp_folder_for_day(day)\n",
    "        localpaths = []\n",
    "        gcs = GcsIO()\n",
    "        if not os.path.isdir(tempfolder):\n",
    "            os.makedirs(tempfolder)\n",
    "        for f in files:\n",
    "            localname = os.path.join(tempfolder, os.path.basename(f))\n",
    "            with gcs.open(f) as gcsfile, open(localname, 'wb') as localfile:\n",
    "                localfile.write(gcsfile.read())\n",
    "                localpaths.append(localname)\n",
    "        return (day, localpaths)\n",
    "    \n",
    "    def build_lst_vrt_files(self,  day, paths):\n",
    "        daytemplate = 'HDF4_EOS:EOS_GRID:\"{}\":MODIS_Grid_8Day_1km_LST:LST_Day_1km'\n",
    "        nighttemplate = 'HDF4_EOS:EOS_GRID:\"{}\":MODIS_Grid_8Day_1km_LST:LST_Night_1km'\n",
    "        daypaths = [daytemplate.format(f) for f in paths]\n",
    "        nightpaths = [nighttemplate.format(f) for f in paths]\n",
    "        thisfolder = os.path.dirname(paths[0])\n",
    "        #day = os.path.basename(thisfolder)\n",
    "        dayvrtfile = os.path.join(thisfolder, \"LST_Day.{}.vrt\".format(day))\n",
    "        nightvrtfile = os.path.join(thisfolder, \"LST_Night.{}.vrt\".format(day))\n",
    "        dayvrt = gdal.BuildVRT(dayvrtfile, daypaths)\n",
    "        dayvrt.FlushCache()\n",
    "        dayvrt = None\n",
    "        nightvrt = gdal.BuildVRT(nightvrtfile, nightpaths)\n",
    "        nightvrt.FlushCache()\n",
    "        nightvrt = None     \n",
    "        return (dayvrtfile, nightvrtfile)\n",
    "        \n",
    "    def expand(self, pcoll):\n",
    "        return (pcoll |  beam.Map(self.localise_day_files)\n",
    "             | \"build_day_night_vrts\" >> beam.MapTuple(lambda d, p: self.build_lst_vrt_files(d,p)) \n",
    "             | \"flat_list_of_vrts\" >> beam.FlatMap(lambda t: t)\n",
    "            )\n",
    "         # groups by day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to run a given calculation against a GDAL raster\n",
    "\n",
    "This is based loosely on gdal_calc.py, it runs in blocks and saves to a sparse output raster. It uses numexpr to allow multithreaded computation of the actual calculation step and it also does multithreaded writing of the output. \n",
    "\n",
    "The calculation should be provided as a string with the input data being called 'band_data' e.g. for MODIS LST use \"band_data * 0.02 + (-273.15)\"\n",
    "e.g. `run_singleband_calculation(\"/tmp/2020.02.02/LST_Day.2020.02.02.vrt\", \"/tmp/2020.02.02/test_celsius_output.tif\", \"band_data * 0.02 + (-273.15)\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the calculated output data\n",
    "\n",
    "Define a PTransform to run that calculation (in fact we will move the function into the PTransform)\n",
    "\n",
    "This will output a file to the worker's local storage, which will still be in the original sinusoidal projection and because it's only going to be read once in the next step, we use the sparse option (incompatible with some reader software) to help keep file size down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslateVrtToLstTiff(beam.PTransform):\n",
    "    \n",
    "    def get_out_name(self, vrtname):\n",
    "        return vrtname.replace('.vrt', '.sinusoidal.tif')\n",
    "    \n",
    "    def expand(self, pColl):\n",
    "        lst_calc = \"band_data * 0.02 + (-273.15)\"\n",
    "        return pColl | beam.Map(lambda v: run_singleband_calculation(v, self.get_out_name(v), lst_calc))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproject the calculated raster\n",
    "\n",
    "Define a PTransform to warp the sinusoidal calculated tiff into the WGS84 compressed output tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateProjectedOutput(beam.PTransform):\n",
    "    \n",
    "    def __init__(self, ForceGlobalExtent = False):\n",
    "        self._forceglobal = ForceGlobalExtent\n",
    "        \n",
    "    def warpfile(self, sinusFile):\n",
    "        cOpts = [\"TILED=YES\", \"BIGTIFF=YES\", \"COMPRESS=LZW\", \"NUM_THREADS=ALL_CPUS\"]\n",
    "        if self._forceglobal:\n",
    "            wo = gdal.WarpOptions(format='GTiff', \n",
    "                          outputBounds=[-180, -90, 180, 90], \n",
    "                          xRes=1/120.0, yRes=-1/120.0, dstSRS='EPSG:4326',\n",
    "                          creationOptions=cOpts, multithread=True, dstNodata=-9999, warpMemoryLimit=4096)\n",
    "        else:\n",
    "            wo = gdal.WarpOptions(format='GTiff', \n",
    "                          xRes=1/120.0, yRes=-1/120.0, dstSRS='EPSG:4326',\n",
    "                          targetAlignedPixels=\"YES\",\n",
    "                          creationOptions=cOpts, multithread=True, dstNodata=-9999, warpMemoryLimit=4096)\n",
    "            \n",
    "        outname = sinusFile.replace('.sinusoidal', '')\n",
    "        gdal.Warp(outname, sinusFile, options=wo)\n",
    "        return outname\n",
    "    \n",
    "    def expand(self, pColl):\n",
    "        return pColl | beam.Map(self.warpfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_bucket_path = bucketpath + '/' + \"output/lst_day\"\n",
    "night_bucket_path = bucketpath + '/' + \"output/lst_night\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally make a PTransform that will upload the output back to the bucket with a mastergrids-formatted filename, then remove the temp files from the worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UploadAndClean(beam.DoFn):\n",
    "    def process(self, finaltif):\n",
    "        date = os.path.basename(os.path.dirname(finaltif))\n",
    "        parts = os.path.basename(finaltif).split('.')\n",
    "        outname = parts[0] + \"_Unfilled.\" + parts[1] + \".\" + parts[2]+parts[3]+\".Data.1km.Data.tif\"\n",
    "        if parts[0].find(\"Day\")>0:\n",
    "            gsPath = day_bucket_path + \"/\" + outname\n",
    "        elif parts[0].find(\"Night\")>0:\n",
    "            gsPath = night_bucket_path + \"/\" + outname\n",
    "        else:\n",
    "            return\n",
    "        gcs = GcsIO()\n",
    "        with gcs.open(gsPath, 'w') as gcsfile, open(finaltif, 'rb') as localfile:\n",
    "            gcsfile.write(localfile.read())\n",
    "        os.remove(finaltif)\n",
    "        yield gsPath\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration interactive pipeline\n",
    "\n",
    "Note that for actually running this on dataflow we will need to move the vrt, calculate, reproject, upload steps into a single PTransform. Otherwise they might (will) get run on separate workers with non-shared local storage and bad things\n",
    "\n",
    "For now, with interactive running, here's a pipeline to put it all together.\n",
    "\n",
    "First check the test cases are set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2019.01.25', '2019.02.02', '2020.01.25', '2020.02.02'],\n",
       " ['h17v03', 'h18v03', 'h17v04', 'h18v04'])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATES_TO_TEST, TILES_TO_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PCollection of the date page urls to kick things off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_page_urls = (beam.Pipeline(InteractiveRunner()) | 'relevant_dates' >> beam.Create(DATES_TO_TEST)\n",
    "                   | 'date_page_urls' >> beam.ParDo(GetDatePageUrl(product_url))\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div id=\"progress_indicator_b5c9637f71ed8facc0ea68c540d37d15\" class=\"spinner-border text-info\" role=\"status\">\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "            .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            .p-Widget.jp-RenderedJavaScript.jp-mod-trusted.jp-OutputArea-output:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            </style>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdn.datatables.net/1.10.20/css/jquery.dataTables.min.css\">\n",
       "            <table id=\"table_df_43d55b1dd1a697d4c8eb78318345ab8d\" class=\"display\" style=\"display:block\"></table>\n",
       "            <script>\n",
       "              \n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\")) {\n",
       "              dt = $(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\").dataTable();\n",
       "            } else if ($(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': '_88.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2019.01.25', 0: 0}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2019.02.02', 0: 1}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2020.01.25', 0: 2}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2020.02.02', 0: 3}])\n",
       "              .draw('full-hold');\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\")) {\n",
       "              dt = $(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\").dataTable();\n",
       "            } else if ($(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': '_88.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2019.01.25', 0: 0}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2019.02.02', 0: 1}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2020.01.25', 0: 2}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2020.02.02', 0: 3}])\n",
       "              .draw('full-hold');\n",
       "          });\n",
       "        }\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            $(\"#progress_indicator_b5c9637f71ed8facc0ea68c540d37d15\").remove();\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            $(\"#progress_indicator_b5c9637f71ed8facc0ea68c540d37d15\").remove();\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\")) {\n",
       "              dt = $(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\").dataTable();\n",
       "            } else if ($(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': '_88.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2019.01.25', 0: 0}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2019.02.02', 0: 1}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2020.01.25', 0: 2}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2020.02.02', 0: 3}])\n",
       "              .draw('full-hold');\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\")) {\n",
       "              dt = $(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\").dataTable();\n",
       "            } else if ($(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_43d55b1dd1a697d4c8eb78318345ab8d\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': '_88.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2019.01.25', 0: 0}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2019.02.02', 0: 1}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2020.01.25', 0: 2}, {1: 'http://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.006/2020.02.02', 0: 3}])\n",
       "              .draw('full-hold');\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ib.show(date_page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download required tiles. This checks what's already on the  bucket, so will return no results if it's already been run and the files are still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hdf_urls = date_page_urls | GetHdfUrlsForDate()\n",
    "#ib.show(hdf_urls)\n",
    "\n",
    "required_for_download = hdf_urls | check_existing_files(hdf_bucket_path, TILES_TO_TEST)\n",
    "\n",
    "download_results = required_for_download | DownloadHdfsToBucket(username, password, HDF_BUCKET_PATH)\n",
    "\n",
    "#ib.show(download_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to wait here\n",
    "\n",
    "vrts = beam.Pipeline(InteractiveRunner()) | beam.Create(DATES_TO_TEST) | CreateVrtsForDays(hdf_bucket_path)\n",
    "uploaded_tiffs = vrts | TranslateVrtToLstTiff() | CreateProjectedOutput() | beam.ParDo(UploadAndClean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(uploaded_tiffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting it to run on dataflow\n",
    "\n",
    "The download part is easier, gdal isn't needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions, SetupOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions(flags=[])\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Sets the Google Cloud Region in which Cloud Dataflow will run.\n",
    "options.view_as(GoogleCloudOptions).region = 'europe-west4'\n",
    "\n",
    "# Because this notebook comes with a locally built version of the Beam Python SDK, we will need to set\n",
    "# the sdk_location option for the Dataflow Runner. You will not need to do this if you are using an\n",
    "# officially released version of Apache Beam.\n",
    "options.view_as(pipeline_options.SetupOptions).sdk_location = (\n",
    "    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' % \n",
    "    beam.version.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_gcs_location = BUCKET_PATH + \"/dataflow\"\n",
    "# Dataflow Staging Location.\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "\n",
    "# Dataflow Temp Location.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_HDF_BUCKET_PATH = BUCKET_PATH + '/hdf_df' #NB not os.path.join as that breaks on windows with backslash paths\n",
    "DF_OUTPUT_BUCKET_PATH = BUCKET_PATH + \"/output_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "p  = beam.Pipeline(DataflowRunner(), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    }
   ],
   "source": [
    "date_page_urls = (p | 'dates_to_process' >> beam.Create(DATES_TO_TEST)\n",
    "                   | 'date_page_urls' >> beam.ParDo(GetDatePageUrl(product_url)))\n",
    "hdf_urls = date_page_urls | GetHdfUrlsForDate()\n",
    "#ib.show(hdf_urls)\n",
    "\n",
    "required_for_download = hdf_urls | check_existing_files(DF_HDF_BUCKET_PATH, TILES_TO_TEST)\n",
    "\n",
    "downloaded_files = required_for_download | DownloadHdfsToBucket(username, password, DF_HDF_BUCKET_PATH)\n",
    "\n",
    "download_result = p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Click <a href=\"https://console.cloud.google.com/dataflow/jobs/europe-west4/2020-06-12_07_22_45-348952328589170505?project=map-oxford-hsg\" target=\"_new\">here</a> for the details of your Dataflow job!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' % \n",
    "      (download_result._job.location, download_result._job.id, download_result._job.projectId))\n",
    "display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still need to fix some import locations (import everything within the function where it's used). Not going to take this further in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### large functions below here to keep notebook a bit tidier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiband_calculation(input_singleband_files, names_in_calc, out_file, calc, out_type='Float32'):\n",
    "    '''Hacked together from gdal_calc.py. Uses numexpr for slightly more efficient calculation (multithreaded).\n",
    "    \n",
    "    input_singleband_files must be a list of dataset paths pointing to single band rasters (or if they are \n",
    "    multiband, the first band will be the one that is read). For multiband rasters a .VRT file could be created \n",
    "    to select the needed band, and passed instead. For HDF files, a string specifying the layer name in the dataset \n",
    "    will be needed.\n",
    "    names_in_calc must be a list of the same length as input_singleband_files, specifying the names given to each of \n",
    "    those datasets in the calc expression e.g. [\"B1\", \"B2\", \"B3\"]\n",
    "    out_file should the output dataset name to create\n",
    "    calc must be a string which will be eval'd against the input data after naming them according to the \n",
    "    terms of names_in_calc i.e. with the example above calc could be \"B1 * 2.0 + B2 / 3.0 + B3\"'''\n",
    "    import numpy as np, numexpr as ne\n",
    "    input_datasets = []\n",
    "    input_bands = []\n",
    "    input_datatypes = []\n",
    "    input_datatype_nums = []\n",
    "    input_ndvs = [] #np.empty(len(input_singleband_files))\n",
    "    DimensionsCheck = None\n",
    "    \n",
    "    for i, input_file in enumerate(input_singleband_files):\n",
    "        ds = gdal.Open(input_file, gdal.GA_ReadOnly)\n",
    "        if not ds:\n",
    "            raise IOError(\"Error opening input file {}\".format(input_file))\n",
    "        input_datasets.append(ds)\n",
    "        input_datatypes.append(gdal.GetDataTypeName(ds.GetRasterBand(1).DataType))\n",
    "        input_datatype_nums.append(ds.GetRasterBand(1).DataType)\n",
    "        input_ndvs.append(ds.GetRasterBand(1).GetNoDataValue())\n",
    "        if DimensionsCheck:\n",
    "            if DimensionsCheck != [ds.RasterXSize, ds.RasterYSize]:\n",
    "                raise Exception(\"Error! Dimensions of file %s (%i, %i) are different from other files (%i, %i).  Cannot proceed\" %\n",
    "                                    (input_file, ds.RasterXSize, ds.RasterYSize, DimensionsCheck[0], DimensionsCheck[1]))\n",
    "        else:\n",
    "            DimensionsCheck = [ds.RasterXSize, ds.RasterYSize]\n",
    "    \n",
    "    if os.path.isfile(out_file):\n",
    "        os.remove(out_file)\n",
    "    # gdal_calc imputes the out type like this, but it isn't valid as e.g. two int datasets can result in a float\n",
    "    # outType = gdal.GetDataTypeName(max(myDataTypeNum))\n",
    "    \n",
    "    #create the output file\n",
    "    outDriver = gdal.GetDriverByName(\"GTiff\")\n",
    "    cOpts =  [\"TILED=YES\", \"SPARSE_OK=TRUE\", \"BLOCKXSIZE=1024\", \"BLOCKYSIZE=1024\", \"BIGTIFF=YES\", \"COMPRESS=LZW\", \"NUM_THREADS=ALL_CPUS\"]\n",
    "    outDS = outDriver.Create(out_file, DimensionsCheck[0], DimensionsCheck[1], 1, gdal.GetDataTypeByName(out_type), cOpts)\n",
    "    outDS.SetGeoTransform(input_datasets[0].GetGeoTransform())\n",
    "    outDS.SetProjection(input_datasets[0].GetProjection())\n",
    "    DefaultNDVLookup = {'Byte': 255, 'UInt16': 65535, 'Int16': -32767, 'UInt32': 4294967293, 'Int32': -2147483647, 'Float32': 3.402823466E+38, 'Float64': 1.7976931348623158E+308}\n",
    "    outBand = outDS.GetRasterBand(1)\n",
    "    outNDV = DefaultNDVLookup[out_type]\n",
    "    outBand.SetNoDataValue(outNDV)\n",
    "    outBand = None\n",
    "    \n",
    "    # vrt file reports a block size of 128*128 but the underlying hdf block size is 1200*100\n",
    "    # so hard code this or some clean multiple : this minimises disk access\n",
    "    myBlockSize = [4800,4800]\n",
    "    nXValid = myBlockSize[0]\n",
    "    nYValid = myBlockSize[1]\n",
    "    nXBlocks = (int)((DimensionsCheck[0] + myBlockSize[0] - 1) / myBlockSize[0]);\n",
    "    nYBlocks = (int)((DimensionsCheck[1] + myBlockSize[1] - 1) / myBlockSize[1]);\n",
    "    \n",
    "    for x in range(0, nXBlocks):\n",
    "        if x == nXBlocks-1:\n",
    "            nXValid = DimensionsCheck[0] - x * myBlockSize[0]\n",
    "        \n",
    "        myX = x * myBlockSize[0]\n",
    "        \n",
    "        nYValid = myBlockSize[1]\n",
    "        myBufSize = nXValid * nYValid\n",
    "        \n",
    "        for y in range(0, nYBlocks):\n",
    "            if y == nYBlocks-1:\n",
    "                nYValid = DimensionsCheck[1] - y * myBlockSize[1]\n",
    "                myBufSize = nXValid * nYValid\n",
    "                \n",
    "            myY = y * myBlockSize[1]\n",
    "            \n",
    "            ndv_locs = None #np.zeros(shape = (len(input_datasets), nYValid, nXValid))\n",
    "            _data3D = np.empty(shape = (len(input_datasets), nYValid, nXValid), \n",
    "                                dtype = gdal.GetDataTypeName(max(input_datatype_nums)))\n",
    "            \n",
    "            for i, calc_name in enumerate(names_in_calc):\n",
    "                _data3D[i] = input_datasets[i].GetRasterBand(1).ReadAsArray(\n",
    "                    xoff=myX, yoff=myY, win_xsize=nXValid, win_ysize=nYValid)\n",
    "                # make a variable named whatever it was specified to be named in the input\n",
    "                # pointing to this band of data in the multiband array. So that the numexpr calc\n",
    "                # can point to a variable of the specified name.\n",
    "                exec(\"%s=_data3D[i]\" %calc_name)\n",
    "                # build a 2d nodata array that's 1 where any of the bands we read are nodata\n",
    "                if input_ndvs[i] is not None:\n",
    "                    if ndv_locs is None:\n",
    "                        ndv_locs = np.zeros(shape=(nYValid, nXValid))\n",
    "                    ndv_locs = 1 * np.logical_or(ndv_locs == 1, _data3D[i] == input_ndvs[i])\n",
    "                \n",
    "            try:\n",
    "                result = ne.evaluate(calc)\n",
    "            except:\n",
    "                print(\"evaluation of calculation failed: \"+calc)\n",
    "                raise\n",
    "            print(result.shape)\n",
    "            if ndv_locs is not None:\n",
    "                # apply ndv (set nodata cells to zero then add nodata value to these cells)\n",
    "                result = ((1 * (ndv_locs==0)) * result + (outNDV * ndv_locs))\n",
    "            print(result.shape)\n",
    "            outBand = outDS.GetRasterBand(1)\n",
    "            outBand.WriteArray(result, xoff=myX, yoff=myY)\n",
    "    return out_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_singleband_calculation(input_singleband_file, out_file, calc, out_type='Float32'):\n",
    "    '''Hacked together from gdal_calc.py. Uses numexpr for slightly more efficient calculation (multithreaded).\n",
    "    \n",
    "    Calc must be the calculation to apply to the data from input_singleband_file, specified as a string which will be \n",
    "    eval'd against the data which will exist in a variable called band_data. i.e. to specify doubling the data then subtracting \n",
    "    three, provide calc=\"(band_data * 2.0) - 3.0\"'''\n",
    "    #input_datasets = []\n",
    "    #myBands = []\n",
    "    #myDataType = []\n",
    "    #myDataTypeNum = []\n",
    "    #myNDV = []\n",
    "    import numpy as np, numexpr as ne\n",
    "    DimensionsCheck = None\n",
    "    \n",
    "    ds = gdal.Open(input_singleband_file, gdal.GA_ReadOnly)\n",
    "    if not ds:\n",
    "        raise IOError(\"Error opening input file {}\".format(input_file))\n",
    "    input_dataset = ds\n",
    "    inputDataType = (gdal.GetDataTypeName(ds.GetRasterBand(1).DataType))\n",
    "    inputDataTypeNum = (ds.GetRasterBand(1).DataType)\n",
    "    inputNDV = (ds.GetRasterBand(1).GetNoDataValue())\n",
    "    \n",
    "    DimensionsCheck = [ds.RasterXSize, ds.RasterYSize]\n",
    "\n",
    "    if os.path.isfile(out_file):\n",
    "        os.remove(out_file)\n",
    "    # gdal_calc does this but it isn't valid as two int datasets can result in a float!\n",
    "    # outType = gdal.GetDataTypeName(max(myDataTypeNum))\n",
    "    \n",
    "    #create the output file\n",
    "    outDriver = gdal.GetDriverByName(\"GTiff\")\n",
    "    cOpts =  [\"TILED=YES\", \"SPARSE_OK=TRUE\", \"BLOCKXSIZE=1024\", \"BLOCKYSIZE=1024\", \"BIGTIFF=YES\", \"COMPRESS=LZW\", \"NUM_THREADS=ALL_CPUS\"]\n",
    "    outDS = outDriver.Create(out_file, DimensionsCheck[0], DimensionsCheck[1], 1, gdal.GetDataTypeByName(out_type), cOpts)\n",
    "    outDS.SetGeoTransform(input_dataset.GetGeoTransform())\n",
    "    outDS.SetProjection(input_dataset.GetProjection())\n",
    "    DefaultNDVLookup = {'Byte': 255, 'UInt16': 65535, 'Int16': -32767, 'UInt32': 4294967293, 'Int32': -2147483647, 'Float32': 3.402823466E+38, 'Float64': 1.7976931348623158E+308}\n",
    "    outBand = outDS.GetRasterBand(1)\n",
    "    outNDV = DefaultNDVLookup[out_type]\n",
    "    outBand.SetNoDataValue(outNDV)\n",
    "    outBand = None\n",
    "    \n",
    "    # vrt file reports a block size of 128*128 but the underlying hdf block size is 1200*100\n",
    "    # so hard code this or some clean multiple : this minimises disk access\n",
    "    myBlockSize = [4800,4800]\n",
    "    nXValid = myBlockSize[0]\n",
    "    nYValid = myBlockSize[1]\n",
    "    nXBlocks = (int)((DimensionsCheck[0] + myBlockSize[0] - 1) / myBlockSize[0]);\n",
    "    nYBlocks = (int)((DimensionsCheck[1] + myBlockSize[1] - 1) / myBlockSize[1]);\n",
    "    \n",
    "    for x in range(0, nXBlocks):\n",
    "        if x == nXBlocks-1:\n",
    "            nXValid = DimensionsCheck[0] - x * myBlockSize[0]\n",
    "        \n",
    "        myX = x * myBlockSize[0]\n",
    "        \n",
    "        nYValid = myBlockSize[1]\n",
    "        myBufSize = nXValid * nYValid\n",
    "        \n",
    "        for y in range(0, nYBlocks):\n",
    "            if y == nYBlocks-1:\n",
    "                nYValid = DimensionsCheck[1] - y * myBlockSize[1]\n",
    "                myBufSize = nXValid * nYValid\n",
    "                \n",
    "            myY = y * myBlockSize[1]\n",
    "            band_data = input_dataset.GetRasterBand(1).ReadAsArray(xoff=myX, yoff=myY, \n",
    "                                                                       win_xsize=nXValid, win_ysize=nYValid)\n",
    "            nodata_locs = band_data == inputNDV\n",
    "            \n",
    "            try:\n",
    "                result = ne.evaluate(calc)\n",
    "            except:\n",
    "                raise\n",
    "            \n",
    "            # apply ndv (set nodata cells to zero then add nodata value to these cells)\n",
    "            result = ((1 * (nodata_locs==0))*result + (outNDV * nodata_locs))\n",
    "            \n",
    "            outBand = outDS.GetRasterBand(1)\n",
    "            outBand.WriteArray(result, xoff=myX, yoff=myY)\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of calculation functions:\n",
    "\n",
    "Both produce a single band output. All input files are single band (or if not then only the first band will be used). The names refer to how many single band input files can be provided. If you have multiband rasters to work on, then first make singleband files from them i.e. .vrt files that select only the relevant band.\n",
    "\n",
    "Single band: \n",
    "\n",
    "`run_singleband_calculation(\"/tmp/2020.02.02/LST_Night.2020.02.02.vrt\", \"/tmp/2020.02.02/test_celsius_night.tif\", \"band_data * 0.02 + (-273.15)\")`\n",
    "\n",
    "Multi band:\n",
    "\n",
    "`\n",
    "run_multiband_calculation([\"/tmp/2020.02.02/LST_Day.2020.02.02.vrt\", \"/tmp/2020.02.02/LST_Night.2020.02.02.vrt\"], \n",
    "                          [\"day\", \"night\"], \n",
    "                          \"/tmp/2020.02.02/test_celsius_diurnaldiff.tif\", \n",
    "                          \"(day * 0.02 + (-273.15)) - (night * 0.02 + (-273.15))\")\n",
    "                          `\n",
    "                          \n",
    "single band function is strictly redundant but just a bit easier to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
