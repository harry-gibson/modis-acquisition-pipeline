{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements\n",
    "\n",
    "The cloud dataflow VMs don't have gdal installed, and it's a bit of a nightmare to do.\n",
    "First install the system binaries for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get --assume-yes install gdal-bin libgdal-dev python3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "!gdal-config --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only then can we successfully pip-install the python GDAL bindings. It needs two environment variables to point it at the system binaries. Then install the same version that apt-get installed of the binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CPLUS_INCLUDE_PATH']=\"/usr/include/gdal\"\n",
    "os.environ['C_INCLUDE_PATH']=\"/usr/include/gdal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdal==2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/osgeo/gdal.py\n"
     ]
    }
   ],
   "source": [
    "!python -c \"from osgeo import gdal;print(gdal.__file__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import sys; print(sys.executable)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/apache-beam-custom/bin/python\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just installed it into whatever python gets called at a shell python command, which looks like it is a conda root environment? That's not the same one as is running this jupyterlab notebook. Don't really understand what the correct way is to install a new module into jupyterlab's environment, but whatever, just hacking it seems to work for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/opt/conda/lib/python3.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrying import retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of setup, time to move on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.io.gcp.gcsio import GcsIO\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ·········\n"
     ]
    }
   ],
   "source": [
    "username='harrygibson'\n",
    "password=getpass()\n",
    "YEAR_FROM = 2019\n",
    "YEAR_TO = 2020\n",
    "DOY_START = 20\n",
    "DOY_END = 40\n",
    "TILE = '*'\n",
    "BASE_URL = \"http://e4ftl01.cr.usgs.gov\"\n",
    "platform = \"MOLT\"\n",
    "product = \"MOD11A2.006\"\n",
    "product_url = f\"{BASE_URL}/{platform}/{product}\"\n",
    "product_url\n",
    "LAYER_TEMPLATE = 'HDF4_EOS:EOS_GRID:\"{}\":MODIS_Grid_8Day_1km_LST:LST_Day_1km'\n",
    "VAR_NAME = \"LST_Day\"\n",
    "\n",
    "# or 'HDF4_EOS:EOS_GRID:\"{}\":MODIS_Grid_8Day_1km_LST:LST_Night_1km'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_PATH = \"gs://hsg-dataflow-test/lst_download_dev\"\n",
    "HDF_BUCKET_PATH = BUCKET_PATH + '/hdf-notebook' #NB not os.path.join as that breaks on windows with backslash paths\n",
    "OUTPUT_BUCKET_PATH = BUCKET_PATH + \"/output-notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-daily dates for use when the product is daily but we only want 8-daily:\n",
    "# days = ['001','009','017','025','033','041','049','057','065','073','081','089','097','105','113','121',\n",
    "#         '129','137','145','153','161','169','177','185','193','201','209','217','225','233','241','249',\n",
    "#         '257','265','273','281','289','297','305','313','321','329','337','345','353','361']\n",
    "# BRDF product:\n",
    "# https://e4ftl01.cr.usgs.gov/MOTA/MCD43D62.006/ thru MCD43D68\n",
    "#  covars = {}\n",
    "# covars['EVI'] = \"((((B2 - B1)*_MODIS_SCALE_CONST) / ((B2 + (B1 * _EVI_C1) - (B3 * _EVI_C2))*_MODIS_SCALE_CONST + _EVI_L) * _EVI_G))\"\n",
    "# covars['TCB'] = \"(B1*TCB0 + B2*TCB1 + B3*TCB2 + B4*TCB3 + B5*TCB4 + B6*TCB5 + B7*TCB6)*_MODIS_SCALE_CONST\"\n",
    "# covars['TCW'] = \"(B1*TCW0 + B2*TCW1 + B3*TCW2 + B4*TCW3 + B5*TCW4 + B6*TCW5 + B7*TCW6)*_MODIS_SCALE_CONST\"\n",
    " # EVI coefficients \n",
    "# _EVI_C1 = 6.0\n",
    "# _EVI_C2 = 7.5\n",
    "# _EVI_L = 1.0 \n",
    "# _EVI_G = 2.5\n",
    "\n",
    "# TCB coefficients \n",
    "#TCB0 = 0.4395\n",
    "#TCB1 = 0.5945\n",
    "#TCB2 = 0.2460\n",
    "#TCB3 = 0.3918\n",
    "#TCB4 = 0.3506\n",
    "#TCB5 = 0.2136\n",
    "#TCB6 = 0.2678\n",
    "\n",
    "# TCW coefficients\n",
    "# TCW0 = 0.1147\n",
    "# TCW1 = 0.2489\n",
    "# TCW2 = 0.2408\n",
    "# TCW3 = 0.3132\n",
    "# TCW4 = -0.3122\n",
    "# TCW5 = -0.6416\n",
    "# TCW6 = -0.5087\n",
    "\n",
    "# scale conversion\n",
    "## THIS CHANGED FROM V5 EVEYTHING WAS SHIT (v5 was 0.0001)\n",
    "#_MODIS_SCALE_CONST = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to parse dates from the MODIS DAAC pages\n",
    "\n",
    "These are local functions, they don't need to run within the Beam pipeline, there's only a single http fetch and then some local list processing. They're more or less taken from get_modis (https://github.com/jgomezdans/get_modis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_selected_dates(year_from=2000, year_to=2020, doy_start=1, doy_end=-1):\n",
    "    import calendar, time\n",
    "    dates = []\n",
    "    for year in range(year_from, year_to+1):\n",
    "        if doy_end == -1:\n",
    "            if calendar.isleap(year):\n",
    "                end_day = 367\n",
    "            else:\n",
    "                end_day = 366\n",
    "        else:\n",
    "            end_day = doy_end\n",
    "        dates_this_yr = [time.strftime(\"%Y.%m.%d\", time.strptime(\"%d/%d\" % (i, year),\n",
    "                                                         \"%j/%Y\")) for i in\n",
    "                 range(doy_start, end_day)]\n",
    "        dates.extend(dates_this_yr)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(out_dir):\n",
    "    # in case we need to do something different to list files on bucket\n",
    "    return os.listdir(out_dir)\n",
    "\n",
    "def load_page_text(url):\n",
    "    import requests, time\n",
    "    # nasa data pools are unavailable for maintenance on wednesday afternoons\n",
    "    the_day_today = time.asctime().split()[0]\n",
    "    the_hour_now = int(time.asctime().split()[3].split(\":\")[0])\n",
    "    if the_day_today == \"Wed\" and 14 <= the_hour_now <= 17:\n",
    "        LOG.info(\"Sleeping for %d hours... Yawn!\" % (18 - the_hour_now))\n",
    "        time.sleep(60 * 60 * (18 - the_hour_now))\n",
    "    resp = requests.get(url)\n",
    "    return resp.text\n",
    "    \n",
    "def parse_modis_dates (product_url, requested_dates, product, out_dir, check_existing_dates=False ):\n",
    "    \"\"\"Parse returned MODIS dates.\n",
    "\n",
    "    This function gets the dates listing for a given MODIS products, and\n",
    "    extracts the dates for when data is available. Further, it crosses these\n",
    "    dates with the required dates that the user has selected and returns the\n",
    "    intersection. Additionally, if the `checkExistingDates` flag is set, we'll check for\n",
    "    files that might already be present in the system and skip them. Note\n",
    "    that if a file failed in downloading, it might still be around\n",
    "    incomplete.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        A top level product URL such as \"http://e4ftl01.cr.usgs.gov/MOTA/MCD45A1.005/\"\n",
    "    dates: list\n",
    "        A list of required dates in the format \"YYYY.MM.DD\"\n",
    "    product: str\n",
    "        The product name, MOD09GA.005\n",
    "    out_dir: str\n",
    "        The output dir\n",
    "    checkExistingDates: bool\n",
    "        Whether to check for present files\n",
    "    Returns\n",
    "    -------\n",
    "    A (sorted) list with the dates that will be downloaded.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    if check_existing_dates:\n",
    "        product = product_url.strip('/').split('/')[-1]\n",
    "        product_no_version = product.split(\".\")[0]\n",
    "        already_here = fnmatch.filter(get_existing_files(out_dir),\n",
    "                                      \"%s*hdf\" % product_no_version)\n",
    "        already_here_dates = [x.split(\".\")[-5][1:]\n",
    "                              for x in already_here]\n",
    "\n",
    "    html = load_page_text(product_url).split('\\n')\n",
    "\n",
    "    available_dates = []\n",
    "    for line in html:\n",
    "        if line.find(\"href\") >= 0 and \\\n",
    "                        line.find(\"[DIR]\") >= 0:\n",
    "            # Points to a directory\n",
    "            the_date = line.split('href=\"')[1].split('\"')[0].strip(\"/\")\n",
    "            if check_existing_dates:\n",
    "                try:\n",
    "                    modis_date = time.strftime(\"%Y%j\",\n",
    "                                               time.strptime(the_date,\n",
    "                                                             \"%Y.%m.%d\"))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if modis_date in already_here_dates:\n",
    "                    continue\n",
    "                else:\n",
    "                    available_dates.append(the_date)\n",
    "            else:\n",
    "                available_dates.append(the_date)\n",
    "\n",
    "    dates = set(requested_dates)\n",
    "    available_dates = set(available_dates)\n",
    "    suitable_dates = list(dates.intersection(available_dates))\n",
    "    suitable_dates.sort()\n",
    "    return suitable_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a list of the dates for which data are required and available\n",
    "\n",
    "This will be the input to our pipeline (possibly along with tiles). We will set up three test cases:\n",
    "* four arbitrary dates\n",
    "* all dates in one year (2019)\n",
    "* all dates available\n",
    "\n",
    "In each case we will generate a list of the string dates then make a pipeline to build them into a PCollection comprising the URLs of the page for the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dates = generate_selected_dates(YEAR_FROM, YEAR_TO, DOY_START, DOY_END)\n",
    "TEST_DATES_SPECIFIC = parse_modis_dates(product_url, real_dates, product, \"C:\\\\temp\")\n",
    "\n",
    "all_2019_dates = generate_selected_dates(2019, 2019, 1, 366)\n",
    "TEST_DATES_WHOLE_YEAR = parse_modis_dates(product_url, all_2019_dates, product, False)\n",
    "\n",
    "all_available_dates = generate_selected_dates(2000, 2020, 1, 366)\n",
    "TEST_DATES_ALL_TIME = parse_modis_dates(product_url, all_available_dates, product, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set up three test cases for which tiles should be downloaded:\n",
    "\n",
    "* A block of four tiles - we'll test this for all available dates\n",
    "* All tiles in Africa - we'll test this for a whole year\n",
    "* All tiles globally - we'll test this for four dates only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TILES_FOUR = ['h17v03', 'h18v03', 'h17v04', 'h18v04']\n",
    "\n",
    "#africa: h16:23, v5:12\n",
    "import itertools\n",
    "TEST_TILES_AFRICA = [f'h{pair[0]:02}v{pair[1]:02}' for pair in (itertools.product(range(16,24), range(5,13)))]\n",
    "\n",
    "TEST_TILES_ALL = '*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which one we're doing\n",
    "\n",
    "MODIFY THIS NEXT CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATES_TO_TEST = [TEST_DATES_SPECIFIC, TEST_DATES_WHOLE_YEAR, TEST_DATES_ALL_TIME][0]\n",
    "TILES_TO_TEST = [TEST_TILES_FOUR, TEST_TILES_AFRICA, TEST_TILES_ALL][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the HDF urls from the date pages\n",
    "\n",
    "For each date, we need to load the page and parse the URLs to the HDF files. Define a PTransform to do this and filter to keep only URLs that are to a tile we need.\n",
    "\n",
    "We return a tuple of (ImageInfo, hdf_url) for each HDF tile to be downloaded. The ImageInfo keys will be duplicated across each tile for the image, i.e. this is a fanout operation. The pipeline should call reshuffle() after applying this transform to allow the many URLs to be downloaded across more machines. It's possible that we ought to do this only for the ones that need to be downloaded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulateHdfUrls(beam.PTransform):\n",
    "                \n",
    "    def load_page_text_to_lines(self, imInfo):\n",
    "        import requests\n",
    "        resp = requests.get(imInfo.HDF_PAGE_URL)\n",
    "        lines = resp.text.split('\\n')\n",
    "        return [(imInfo, l) for l in lines]\n",
    "        #return beam.Create(lines)\n",
    "    \n",
    "    def parse_hdf_from_line(self, imInfo, textline):\n",
    "        if textline.find('.hdf\"') != -1:\n",
    "            return (imInfo, imInfo.HDF_PAGE_URL + '/' + textline.split('<a href=\"')[1].split('\">')[0])\n",
    "    \n",
    "    def check_tile_is_needed(self, imInfo, url):\n",
    "        import os\n",
    "        thistile = os.path.basename(url).split(\".\")[2]\n",
    "        return imInfo.TILES == \"*\" or thistile in imInfo.TILES\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (pcoll\n",
    "                | \"Load_page_lines\" >> beam.FlatMap(lambda imInfo: self.load_page_text_to_lines(imInfo))\n",
    "                #| \"Flatten\" >> beam.Flatten()\n",
    "                | \"discover_hdf_urls\" >> beam.MapTuple(lambda imInfo, line:self.parse_hdf_from_line(imInfo, line))\n",
    "                | \"remove_non_matching_lines\" >> beam.Filter(lambda l: l is not None)\n",
    "                | \"remove_non_required_tile_urls\" >> beam.Filter(lambda info_url_tpl: self.check_tile_is_needed(info_url_tpl[0], info_url_tpl[1]))\n",
    "               )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the files\n",
    "\n",
    "We have a list of HDF URLs that are needed for the given day. Now get a URL for each of them on the storage bucket. This means checking the bucket to see if we've already downloaded it (when producing another image from the same set of HDFs) and downloading it to the bucket from NASA if not. Then returning the bucket path.\n",
    "\n",
    "Ideally this should be developed to take account of the version part of their filename i.e. note if we already have a tile but of an earlier version and do {something}. At present we just look for matching tile and date.\n",
    "\n",
    "The PTransform class maintains an authenticated session object as a member variable as i guess this might be better than logging in for every tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMissingHdfsToBucket(beam.PTransform):\n",
    "    # Take a PCollection of (ImInfo, One HDF URL required for image) tuples\n",
    "    # return a PCollection of (ImInfo, GCS path to HDF) tuples for all HDFs needed for this date image\n",
    "    # The ImInfos are duplicated i.e. one per download url, this is so that the downloads can spread \n",
    "    # more broadly.\n",
    "    # If they are already on the bucket we return the bucket path; if not we download to bucket THEN \n",
    "    # return the bucket path\n",
    "    \n",
    "    def __init__(self, hdf_bucket_path, nasa_user, nasa_pw):\n",
    "        import requests\n",
    "        gcs = GcsIO()\n",
    "        self._existing_file_paths = list(gcs.list_prefix(hdf_bucket_path).keys())\n",
    "        self._existing_file_names = [os.path.basename(l) for l in self._existing_file_paths]\n",
    "        \n",
    "        self._session = requests.Session()\n",
    "        self._session.auth = (nasa_user, nasa_pw)\n",
    "        self._hdf_bucket_path = hdf_bucket_path\n",
    "        self._chunk_size = 8 * 1024 * 1024\n",
    "        \n",
    "    def bucket_path_if_existing(self, iminfo, url):\n",
    "        filename = os.path.basename(url)\n",
    "        matching_on_bucket = [bp for bp in self._existing_file_paths if filename in bp]\n",
    "        if len(matching_on_bucket) > 0:\n",
    "            return (iminfo, matching_on_bucket[0], True)\n",
    "        else:\n",
    "            return (iminfo, url, False)\n",
    "        \n",
    "    @retry(stop_max_attempt_number=7, wait_fixed=3000)\n",
    "    def download_file(self, iminfo, url):\n",
    "        import requests, tempfile, os\n",
    "        from apache_beam.io.gcp.gcsio import GcsIO\n",
    "        req = self._session.request('get', url)\n",
    "        resp = self._session.get(req.url, stream=True)\n",
    "        product, datestr, fname = url.split('/')[-3:]\n",
    "        bucketpath = '/'.join([self._hdf_bucket_path, product, datestr, fname])\n",
    "        gcs = GcsIO()\n",
    "        with gcs.open(bucketpath, 'w') as fp:\n",
    "        #with open(tempfilename, 'wb') as fp:\n",
    "            for chunk in resp.iter_content(chunk_size=self._chunk_size):\n",
    "                if chunk:\n",
    "                    fp.write(chunk)\n",
    "            fp.flush()\n",
    "            #os.fsync(fp)\n",
    "        return (iminfo, bucketpath)\n",
    "    \n",
    "    def expand(self, pcoll): #(iminfo, hdfurl)\n",
    "        # get tuple of (iminfo, bucketpath, True) if already done, or retain (iminfo, url, False) if not\n",
    "        pre_existing_bucket_paths = (pcoll | \"convert_to_gcs_paths\" >> beam.MapTuple(self.bucket_path_if_existing))\n",
    "        # filter into a pcoll of existing and a pcoll needing download\n",
    "        to_copy = pre_existing_bucket_paths | beam.Filter(lambda d: d[2]) | beam.Map(lambda t: (t[0], t[1]))\n",
    "        to_download = pre_existing_bucket_paths | beam.Filter(lambda d: not d[2]) | beam.Map(lambda t: (t[0], t[1]))\n",
    "        download_results = to_download | beam.MapTuple(self.download_file) \n",
    "        all_bucket_paths = (to_copy, download_results) | beam.Flatten() \n",
    "        return all_bucket_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Virtually) mosaic the files\n",
    "\n",
    "Build a GDAL vrt for each image's tiles to mosaic them together. This requires knowledge of which layer needs to be extracted from the HDF, so that info is recorded on the ImageInfo elements.\n",
    "\n",
    "Although GDAL can in theory read inputs straight from bucket storage (/vsigs/) this isn't necessarily built in and needs a different authentication flow so would be a lot more complex. Instead we copy the files back to worker's local storage and work on them there.\n",
    "\n",
    "This transform needs to be called once for every output image, so after running the download step the results need to be grouped by key (ImageInfo) before running this transform.\n",
    "\n",
    "This transform probably needs to be replaced with a DoFn (and include the subsequent tiff production one, too) to ensure that the localise/vrt/calculate/reproject steps for a single image all run on the same machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildVrt(beam.PTransform):\n",
    "    def __init__(self):\n",
    "        import os\n",
    "        from apache_beam.io.gcp.gcsio import GcsIO\n",
    "        #self._hdfpath = bucketpath\n",
    "        #gcs = GcsIO()\n",
    "        #self._existing = [l for l in list(gcs.list_prefix(hdf_bucket_path).keys())]\n",
    "            \n",
    "    def get_tmp_folder_for_day(self, day):\n",
    "        tmpfolder = tempfile.gettempdir()\n",
    "        workfolder = os.path.join(tmpfolder, day)\n",
    "        return workfolder\n",
    "    \n",
    "    def localise_day_files(self, iminfo):\n",
    "        day = iminfo.DATE_STR\n",
    "        gspaths  = iminfo.HDF_GS_PATHS\n",
    "        tempfolder = self.get_tmp_folder_for_day(day)\n",
    "        localpaths = []\n",
    "        gcs = GcsIO()\n",
    "        if not os.path.isdir(tempfolder):\n",
    "            os.makedirs(tempfolder)\n",
    "        for f in gspaths:\n",
    "            localname = os.path.join(tempfolder, os.path.basename(f))\n",
    "            if not os.path.exists(localname):\n",
    "                with gcs.open(f) as gcsfile, open(localname, 'wb') as localfile:\n",
    "                    localfile.write(gcsfile.read())\n",
    "            localpaths.append(localname)\n",
    "        iminfo.HDF_LOCAL_PATHS=localpaths\n",
    "        return iminfo\n",
    "    \n",
    "    def build_vrt_file(self, iminfo):\n",
    "        gdaltemplate = iminfo.HDF_LYR_TEMPLATE\n",
    "        localpaths = iminfo.HDF_LOCAL_PATHS\n",
    "        lyrpaths = [gdaltemplate.format(f) for f in localpaths]\n",
    "        tempfolder = os.path.dirname(localpaths[0])\n",
    "        vrtfilename = os.path.join(tempfolder, \"{}.{}.vrt\".format(\n",
    "            iminfo.VRT_VAR_NAME, iminfo.DATE_STR))\n",
    "        vrtobj = gdal.BuildVRT(vrtfilename, lyrpaths)\n",
    "        vrtobj.FlushCache()\n",
    "        vrtobj = None\n",
    "        iminfo.VRT_LOCAL_PATH = vrtfilename\n",
    "        return iminfo\n",
    "        \n",
    "    def expand(self, pcoll): # iminfo objects populated with gs paths\n",
    "        return (pcoll | \"copy_files_to_local\" >> beam.Map(self.localise_day_files)\n",
    "             | \"build_vrts_on_local\" >> beam.Map(self.build_vrt_file) \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateCalculatedOutput(beam.PTransform):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the calculated output data\n",
    "\n",
    "Define a PTransform to run that calculation (in fact we will move the function into the PTransform)\n",
    "\n",
    "This will output a file to the worker's local storage, which will still be in the original sinusoidal projection and because it's only going to be read once in the next step, we use the sparse option (incompatible with some reader software) to help keep file size down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproject the calculated raster\n",
    "\n",
    "Define a PTransform to warp the sinusoidal calculated tiff into the WGS84 compressed output tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateProjectedOutput(beam.PTransform):\n",
    "    \n",
    "    def __init__(self, ForceGlobalExtent = False):\n",
    "        self._forceglobal = ForceGlobalExtent\n",
    "        \n",
    "    def warpfile(self, sinusFile):\n",
    "        cOpts = [\"TILED=YES\", \"BIGTIFF=YES\", \"COMPRESS=LZW\", \"NUM_THREADS=ALL_CPUS\"]\n",
    "        if self._forceglobal:\n",
    "            wo = gdal.WarpOptions(format='GTiff', \n",
    "                          outputBounds=[-180, -90, 180, 90], \n",
    "                          xRes=1/120.0, yRes=-1/120.0, dstSRS='EPSG:4326',\n",
    "                          creationOptions=cOpts, multithread=True, dstNodata=-9999, warpMemoryLimit=4096)\n",
    "        else:\n",
    "            wo = gdal.WarpOptions(format='GTiff', \n",
    "                          xRes=1/120.0, yRes=-1/120.0, dstSRS='EPSG:4326',\n",
    "                          targetAlignedPixels=\"YES\",\n",
    "                          creationOptions=cOpts, multithread=True, dstNodata=-9999, warpMemoryLimit=4096)\n",
    "            \n",
    "        outname = sinusFile.replace('.sinusoidal', '')\n",
    "        gdal.Warp(outname, sinusFile, options=wo)\n",
    "        return outname\n",
    "    \n",
    "    def expand(self, pColl):\n",
    "        return pColl | beam.Map(self.warpfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_bucket_path = OUTPUT_BUCKET_PATH + '/lst_day'\n",
    "night_bucket_path = OUTPUT_BUCKET_PATH + '/lst_night'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally make a PTransform that will upload the output back to the bucket with a mastergrids-formatted filename, then remove the temp files from the worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UploadAndClean(beam.DoFn):\n",
    "    def process(self, finaltif):\n",
    "        date = os.path.basename(os.path.dirname(finaltif))\n",
    "        parts = os.path.basename(finaltif).split('.')\n",
    "        outname = parts[0] + \"_Unfilled.\" + parts[1] + \".\" + parts[2]+parts[3]+\".Data.1km.Data.tif\"\n",
    "        if parts[0].find(\"Day\")>0:\n",
    "            gsPath = day_bucket_path + \"/\" + outname\n",
    "        elif parts[0].find(\"Night\")>0:\n",
    "            gsPath = night_bucket_path + \"/\" + outname\n",
    "        else:\n",
    "            return\n",
    "        gcs = GcsIO()\n",
    "        with gcs.open(gsPath, 'w') as gcsfile, open(finaltif, 'rb') as localfile:\n",
    "            gcsfile.write(localfile.read())\n",
    "        os.remove(finaltif)\n",
    "        yield gsPath\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class that will hold all the info needed to create a single output image and will become the \"things\" that comprise the PCollection that runs through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VrtImageInfo():\n",
    "    # defines the information necessary to create a single virtual mosaic image consisting of \n",
    "    # one hdf page url, \n",
    "    # one date (corresponding to the given hdf page url(s))\n",
    "    # one list of tiles (or '*')\n",
    "    # one hdf layer template, used to build a vrt from the hdf tiles \n",
    "    #  retrieved from the hdf page url\n",
    "    # variable name for the output image e.g. LST_Day, EVI\n",
    "    \n",
    "    def __init__(self, hdf_page_url, date, tiles, hdf_layer_template, band_name):\n",
    "        # items which are fixed at creation according to the job that's being run\n",
    "        self.HDF_PAGE_URL = hdf_page_url\n",
    "        self.DATE_STR = date\n",
    "        self.TILES = tiles\n",
    "        self.HDF_LYR_TEMPLATE = hdf_layer_template\n",
    "        self.VRT_VAR_NAME=band_name\n",
    "        # items which will be populated along the pipeline\n",
    "        self.HDF_URLS = None\n",
    "        self.HDF_GS_PATHS = None\n",
    "        self.HDF_LOCAL_PATHS = None\n",
    "        self.VRT_LOCAL_PATH = None\n",
    "        \n",
    "    def as_tuple(self):\n",
    "        return (self.DATE_STR, self.VRT_VAR_NAME, self.HDF_LYR_TEMPLATE, self.TILES, self.HDF_PAGE_URL)\n",
    "\n",
    "    def __str__(self): # just override this to make it easier to ib.show for debugging\n",
    "        return \"{}\\n{}\\n{}\\n|\\nDownloaded: {}\\nCopied: {}\".format(\n",
    "            self.DATE_STR, self.TILES, self.VRT_VAR_NAME, \n",
    "        self.HDF_GS_PATHS is not None, self.HDF_LOCAL_PATHS is not None)\n",
    "\n",
    "class CalculatedImageInfo():\n",
    "    # defines the information necessary to create a single calculated output image \n",
    "    # from one or more virtual input images (VrtImageInfo)\n",
    "        def __init__(self, date, input_vrt_objs, calc, min_clip, max_clip, out_var_name, out_type='Float32'):\n",
    "            self.DATE_STR = date\n",
    "            self.INPUT_VRT_INFOS = input_vrt_objs\n",
    "            #self.NAMES_IN_CALC = names_in_calc,\n",
    "            self.CALC = calc\n",
    "            self.MIN_CLIP_VAL = min_clip\n",
    "            self.MAX_CLIP_VAL = max_clip\n",
    "            self.TIF_VAR_NAME = out_var_name\n",
    "            self.OUT_TYPE = out_type\n",
    "            assert len(set([vInfo.DATE_STR for vInfo in input_vrt_objs]))==1\n",
    "        \n",
    "        def run_multiband_calculation(self, out_file):\n",
    "            '''Hacked together from gdal_calc.py. Uses numexpr for slightly more efficient calculation (multithreaded).\n",
    "\n",
    "            input_singleband_files must be a list of dataset paths pointing to single band rasters (or if they are \n",
    "            multiband, the first band will be the one that is read). For multiband rasters a .VRT file could be created \n",
    "            to select the needed band, and passed instead. For HDF files, a string specifying the layer name in the dataset \n",
    "            will be needed.\n",
    "            names_in_calc must be a list of the same length as input_singleband_files, specifying the names given to each of \n",
    "            those datasets in the calc expression e.g. [\"B1\", \"B2\", \"B3\"]\n",
    "            out_file should the output dataset name to create\n",
    "            calc must be a string which will be eval'd against the input data after naming them according to the \n",
    "            terms of names_in_calc i.e. with the example above calc could be \"B1 * 2.0 + B2 / 3.0 + B3\"'''\n",
    "            import numpy as np, numexpr as ne\n",
    "            input_datasets = []\n",
    "            input_bands = []\n",
    "            input_datatypes = []\n",
    "            input_datatype_nums = []\n",
    "            input_ndvs = [] #np.empty(len(input_singleband_files))\n",
    "            DimensionsCheck = None\n",
    "            \n",
    "            input_singleband_files = [v.VRT_LOCAL_PATH for v in self.INPUT_VRT_INFOS]\n",
    "            names_in_calc = [v.VRT_VAR_NAME for v in self.INPUT_VRT_INFOS]\n",
    "            calc = self.CALC\n",
    "            out_type = self.OUT_TYPE\n",
    "            min_clip = self.MIN_CLIP_VAL\n",
    "            max_clip = self.MAX_CLIP_VAL\n",
    "            \n",
    "            for i, input_file in enumerate(input_singleband_files):\n",
    "                ds = gdal.Open(input_file, gdal.GA_ReadOnly)\n",
    "                if not ds:\n",
    "                    raise IOError(\"Error opening input file {}\".format(input_file))\n",
    "                input_datasets.append(ds)\n",
    "                input_datatypes.append(gdal.GetDataTypeName(ds.GetRasterBand(1).DataType))\n",
    "                input_datatype_nums.append(ds.GetRasterBand(1).DataType)\n",
    "                input_ndvs.append(ds.GetRasterBand(1).GetNoDataValue())\n",
    "                if DimensionsCheck:\n",
    "                    if DimensionsCheck != [ds.RasterXSize, ds.RasterYSize]:\n",
    "                        raise Exception(\"Error! Dimensions of file %s (%i, %i) are different from other files (%i, %i).  Cannot proceed\" %\n",
    "                                            (input_file, ds.RasterXSize, ds.RasterYSize, DimensionsCheck[0], DimensionsCheck[1]))\n",
    "                else:\n",
    "                    DimensionsCheck = [ds.RasterXSize, ds.RasterYSize]\n",
    "\n",
    "            if os.path.isfile(out_file):\n",
    "                os.remove(out_file)\n",
    "            # gdal_calc imputes the out type like this, but it isn't valid as e.g. two int datasets can result in a float\n",
    "            # outType = gdal.GetDataTypeName(max(myDataTypeNum))\n",
    "\n",
    "            #create the output file\n",
    "            outDriver = gdal.GetDriverByName(\"GTiff\")\n",
    "            cOpts =  [\"TILED=YES\", \"SPARSE_OK=TRUE\", \"BLOCKXSIZE=1024\", \"BLOCKYSIZE=1024\", \"BIGTIFF=YES\", \"COMPRESS=LZW\", \"NUM_THREADS=ALL_CPUS\"]\n",
    "            outDS = outDriver.Create(out_file, DimensionsCheck[0], DimensionsCheck[1], 1, gdal.GetDataTypeByName(out_type), cOpts)\n",
    "            outDS.SetGeoTransform(input_datasets[0].GetGeoTransform())\n",
    "            outDS.SetProjection(input_datasets[0].GetProjection())\n",
    "            DefaultNDVLookup = {'Byte': 255, 'UInt16': 65535, 'Int16': -32767, 'UInt32': 4294967293, 'Int32': -2147483647, 'Float32': 3.402823466E+38, 'Float64': 1.7976931348623158E+308}\n",
    "            outBand = outDS.GetRasterBand(1)\n",
    "            outNDV = DefaultNDVLookup[out_type]\n",
    "            outBand.SetNoDataValue(outNDV)\n",
    "            outBand = None\n",
    "\n",
    "            # vrt file reports a block size of 128*128 but the underlying hdf block size is 1200*100\n",
    "            # so hard code this or some clean multiple : this minimises disk access\n",
    "            myBlockSize = [4800,4800]\n",
    "            nXValid = myBlockSize[0]\n",
    "            nYValid = myBlockSize[1]\n",
    "            nXBlocks = (int)((DimensionsCheck[0] + myBlockSize[0] - 1) / myBlockSize[0]);\n",
    "            nYBlocks = (int)((DimensionsCheck[1] + myBlockSize[1] - 1) / myBlockSize[1]);\n",
    "\n",
    "            for x in range(0, nXBlocks):\n",
    "                if x == nXBlocks-1:\n",
    "                    nXValid = DimensionsCheck[0] - x * myBlockSize[0]\n",
    "\n",
    "                myX = x * myBlockSize[0]\n",
    "\n",
    "                nYValid = myBlockSize[1]\n",
    "                myBufSize = nXValid * nYValid\n",
    "\n",
    "                for y in range(0, nYBlocks):\n",
    "                    if y == nYBlocks-1:\n",
    "                        nYValid = DimensionsCheck[1] - y * myBlockSize[1]\n",
    "                        myBufSize = nXValid * nYValid\n",
    "\n",
    "                    myY = y * myBlockSize[1]\n",
    "\n",
    "                    ndv_locs = None #np.zeros(shape = (len(input_datasets), nYValid, nXValid))\n",
    "                    _data3D = np.empty(shape = (len(input_datasets), nYValid, nXValid), \n",
    "                                        dtype = gdal.GetDataTypeName(max(input_datatype_nums)))\n",
    "\n",
    "                    for i, calc_name in enumerate(names_in_calc):\n",
    "                        _data3D[i] = input_datasets[i].GetRasterBand(1).ReadAsArray(\n",
    "                            xoff=myX, yoff=myY, win_xsize=nXValid, win_ysize=nYValid)\n",
    "                        # make a variable named whatever it was specified to be named in the input\n",
    "                        # pointing to this band of data in the multiband array. So that the numexpr calc\n",
    "                        # can point to a variable of the specified name.\n",
    "                        exec(\"%s=_data3D[i]\" %calc_name)\n",
    "                        # build a 2d nodata array that's 1 where any of the bands we read are nodata\n",
    "                        if input_ndvs[i] is not None:\n",
    "                            if ndv_locs is None:\n",
    "                                ndv_locs = np.zeros(shape=(nYValid, nXValid))\n",
    "                            ndv_locs = 1 * np.logical_or(ndv_locs == 1, _data3D[i] == input_ndvs[i])\n",
    "\n",
    "                    try:\n",
    "                        result = ne.evaluate(calc)\n",
    "                    except:\n",
    "                        print(\"evaluation of calculation failed: \"+calc)\n",
    "                        raise\n",
    "                    print(result.shape)\n",
    "                    if ndv_locs is not None:\n",
    "                        # apply ndv (set nodata cells to zero then add nodata value to these cells)\n",
    "                        result = ((1 * (ndv_locs==0)) * result + (outNDV * ndv_locs))\n",
    "                    print(result.shape)\n",
    "                    outBand = outDS.GetRasterBand(1)\n",
    "                    outBand.WriteArray(result, xoff=myX, yoff=myY)\n",
    "            return out_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration interactive pipeline\n",
    "\n",
    "Note that for actually running this on dataflow we will need to move the vrt, calculate, reproject, upload steps into a single PTransform. Otherwise they might (will) get run on separate workers with non-shared local storage and bad things\n",
    "\n",
    "For now, with interactive running, here's a pipeline to put it all together.\n",
    "\n",
    "First check the test cases are set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2019.01.25', '2019.02.02', '2020.01.25', '2020.02.02'],\n",
       " ['h17v03', 'h18v03', 'h17v04', 'h18v04'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATES_TO_TEST, TILES_TO_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h17v03', 'h18v03', 'h17v04', 'h18v04', 'h18v05']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extratile = TILES_TO_TEST.copy()\n",
    "extratile.append('h18v05')\n",
    "extratile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PCollection of the required images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the itertools\n",
    "tiles = [TILES_TO_TEST]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some classes to map the requested data products and dates into the output images we want to create. \n",
    "\n",
    "Each output image (say EVI for 2020.01.01 or LST_Day for 2019.02.02) will be comprised of one or more input singleband vrt images, defining the raw data layers that go into calculating it. A calculation is also provided to transform from the raw input data to the output. For example to go from integerised and scaled temperature representations in Kelvin as stored in MOD11A2 into the required output (float temperature in celsius). Or to go from the 0-32000 integer values in each of the 7 BRDF bands to a single calculated float value of TCB. Each output image will be one job in the pipeline, represented by a `CalculatedImageInfo` object.\n",
    "\n",
    "In turn, each singleband vrt image will be comprised of a page to download HDFs from; (eventually) a list of HDF tile files to obtain to make the image (either 1 or 317 per image, depending on MODIS product); and a layer template to extract the required subdataset from each HDF (MOD11A2 HDFs contain several bands / subdatasets; others don't). Each vrt image will be represented by a `VrtImageInfo` object and will provide a list of HDF files for download.\n",
    "\n",
    "We provide 2 CSV files as input. `bands.csv` contains the necessary info to obtain the downloads for each band (product url and layer template) along with a band identifier. `products.csv` contains the necessary info to construct a `CalculatedImageInfo` for a particular date and region; i.e. the IDs of the bands required and the calculation to extract the needed numbers. For each of these, we make a factory object which will make the image specification objects for the required day and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleBandFactory:\n",
    "    def __init__(self, line):\n",
    "        self.id, self._product_url, self._layer_template = line.strip().split(',')\n",
    "        \n",
    "    def CreateVrtSpecificationForDayAndRegion(self, date, tiles):\n",
    "        return VrtImageInfo(self._product_url + \"/\" + date, date, tiles, self._layer_template, self.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../resources/bands.csv\") as f:\n",
    "    lines = (f.readlines())\n",
    "BAND_FACTORIES = [SingleBandFactory(l) for l in lines[1:]]    \n",
    "#v = BAND_FACTORIES[2].CreateVrtSpecification('2020.02.02', '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputImageSpecFactory:\n",
    "    def __init__(self, line):\n",
    "        var_name, bands_unsplit, calc, min_clip, max_clip = line.split(',')\n",
    "        bands = bands_unsplit.split(';')\n",
    "        self.id = var_name\n",
    "        self._calc = calc\n",
    "        self._min_clip = min_clip\n",
    "        self._max_clip = max_clip\n",
    "        self.vrt_band_factories = [i for i in BAND_FACTORIES if i.id in bands]\n",
    "        if len(bands) != len(self.vrt_band_factories): \n",
    "            # fail if a band was mentioned that wasn't in the bands.csv instructions\n",
    "            assert False\n",
    "        \n",
    "    def CreateOutputImageSpecification(self, date, tiles):\n",
    "        vrtObjs = [v.CreateVrtSpecificationForDayAndRegion(date, tiles) for v in self.vrt_band_factories]\n",
    "        return CalculatedImageInfo(date, vrtObjs, self._calc, self._min_clip, self._max_clip, self.id)\n",
    "                \n",
    "#def __init__(self, date, input_vrt_objs, calc, min_clip, max_clip, out_var_name, out_type='Float32'):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../resources/products.csv\") as f:\n",
    "    lines = (f.readlines())\n",
    "IMAGE_SPEC_FACTORIES = [OutputImageSpecFactory(l) for l in lines[1:]]\n",
    "#i  = IMAGE_SPEC_FACTORIES[0].CreateOutputImageSpecification('2020.02.02', '*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make one output image specification for every requested product (from products.csv) and every date required, for the given region.\n",
    "Zip them all together and make an image spec from each factory for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [t[0].CreateOutputImageSpecification(t[1], t[2]) for t in (itertools.product(IMAGE_SPEC_FACTORIES, DATES_TO_TEST, tiles))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialJobsPColl = p | beam.Create(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div id=\"progress_indicator_45b6fc8a67e7e79b581cfe2ea5efd4ff\" class=\"spinner-border text-info\" role=\"status\">\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "            .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            .p-Widget.jp-RenderedJavaScript.jp-mod-trusted.jp-OutputArea-output:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            </style>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdn.datatables.net/1.10.20/css/jquery.dataTables.min.css\">\n",
       "            <table id=\"table_df_391f2c4228744e4347c364fc03811286\" class=\"display\" style=\"display:block\"></table>\n",
       "            <script>\n",
       "              \n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_391f2c4228744e4347c364fc03811286\")) {\n",
       "              dt = $(\"#table_df_391f2c4228744e4347c364fc03811286\").dataTable();\n",
       "            } else if ($(\"#table_df_391f2c4228744e4347c364fc03811286_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_391f2c4228744e4347c364fc03811286\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'initialJobsPColl.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: '<__main__.CalculatedImageInfo object at 0x7f0b0e18b4d0>', 0: 0}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e18b150>', 0: 1}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e192bd0>', 0: 2}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e192250>', 0: 3}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1958d0>', 0: 4}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e18be10>', 0: 5}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e187f90>', 0: 6}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e19dc90>', 0: 7}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e17e6d0>', 0: 8}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e17e4d0>', 0: 9}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e17e050>', 0: 10}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f37eb10>', 0: 11}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e19f890>', 0: 12}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e196890>', 0: 13}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e186b90>', 0: 14}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1869d0>', 0: 15}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f37a190>', 0: 16}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f377d10>', 0: 17}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f370d90>', 0: 18}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f370650>', 0: 19}])\n",
       "              .draw('full-hold');\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_391f2c4228744e4347c364fc03811286\")) {\n",
       "              dt = $(\"#table_df_391f2c4228744e4347c364fc03811286\").dataTable();\n",
       "            } else if ($(\"#table_df_391f2c4228744e4347c364fc03811286_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_391f2c4228744e4347c364fc03811286\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'initialJobsPColl.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: '<__main__.CalculatedImageInfo object at 0x7f0b0e18b4d0>', 0: 0}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e18b150>', 0: 1}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e192bd0>', 0: 2}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e192250>', 0: 3}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1958d0>', 0: 4}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e18be10>', 0: 5}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e187f90>', 0: 6}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e19dc90>', 0: 7}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e17e6d0>', 0: 8}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e17e4d0>', 0: 9}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e17e050>', 0: 10}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f37eb10>', 0: 11}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e19f890>', 0: 12}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e196890>', 0: 13}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e186b90>', 0: 14}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1869d0>', 0: 15}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f37a190>', 0: 16}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f377d10>', 0: 17}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f370d90>', 0: 18}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f370650>', 0: 19}])\n",
       "              .draw('full-hold');\n",
       "          });\n",
       "        }\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            $(\"#progress_indicator_45b6fc8a67e7e79b581cfe2ea5efd4ff\").remove();\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            $(\"#progress_indicator_45b6fc8a67e7e79b581cfe2ea5efd4ff\").remove();\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_391f2c4228744e4347c364fc03811286\")) {\n",
       "              dt = $(\"#table_df_391f2c4228744e4347c364fc03811286\").dataTable();\n",
       "            } else if ($(\"#table_df_391f2c4228744e4347c364fc03811286_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_391f2c4228744e4347c364fc03811286\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'initialJobsPColl.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: '<__main__.CalculatedImageInfo object at 0x7f0b0f351210>', 0: 0}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f34d550>', 0: 1}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f370850>', 0: 2}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f359f10>', 0: 3}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f370490>', 0: 4}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f363e50>', 0: 5}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f36d250>', 0: 6}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f34ddd0>', 0: 7}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1aeb50>', 0: 8}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1aed10>', 0: 9}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1a33d0>', 0: 10}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1a3590>', 0: 11}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1ae990>', 0: 12}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d949b10>', 0: 13}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d949b90>', 0: 14}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d946bd0>', 0: 15}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d946c50>', 0: 16}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d948c90>', 0: 17}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d948d10>', 0: 18}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d94bd50>', 0: 19}])\n",
       "              .draw('full-hold');\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_391f2c4228744e4347c364fc03811286\")) {\n",
       "              dt = $(\"#table_df_391f2c4228744e4347c364fc03811286\").dataTable();\n",
       "            } else if ($(\"#table_df_391f2c4228744e4347c364fc03811286_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_391f2c4228744e4347c364fc03811286\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'initialJobsPColl.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: '<__main__.CalculatedImageInfo object at 0x7f0b0f351210>', 0: 0}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f34d550>', 0: 1}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f370850>', 0: 2}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f359f10>', 0: 3}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f370490>', 0: 4}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f363e50>', 0: 5}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f36d250>', 0: 6}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0f34ddd0>', 0: 7}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1aeb50>', 0: 8}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1aed10>', 0: 9}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1a33d0>', 0: 10}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1a3590>', 0: 11}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0e1ae990>', 0: 12}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d949b10>', 0: 13}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d949b90>', 0: 14}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d946bd0>', 0: 15}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d946c50>', 0: 16}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d948c90>', 0: 17}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d948d10>', 0: 18}, {1: '<__main__.CalculatedImageInfo object at 0x7f0b0d94bd50>', 0: 19}])\n",
       "              .draw('full-hold');\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ib.show(initialJobsPColl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the HDF URLs matching the required dates and tiles (regardless of whether they've already been downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_urls = initialJobsPColl | PopulateHdfUrls()\n",
    "#ib.show(hdf_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshuffled_urls = hdf_urls | \"Fusion break\" >> beam.Reshuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the paths to these tiles on the GS bucket - downloading them if they aren't already there. Hopefully the reshuffle will mean that dataflow will scale this up as there may be 317 times more urls to download than there are images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_paths = reshuffled_urls | GetMissingHdfsToBucket(HDF_BUCKET_PATH, username, password)\n",
    "#ib.show(bucket_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InflateImInfoWithGsPaths(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self, tpl):\n",
    "        iminfo = tpl[0]\n",
    "        gsfiles = tpl[1]\n",
    "        iminfo.HDF_GS_PATHS = gsfiles\n",
    "        return [iminfo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate back to one element per output image, collecting all the tile paths to a list then storing it as a field on the ImInfo element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_info_with_tile_paths = (bucket_paths \n",
    "        | \"Concatenate info to HDF\" >> beam.GroupByKey() \n",
    "        | \"Insert GSpaths\" >> beam.ParDo(InflateImInfoWithGsPaths()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div id=\"progress_indicator_83d15d4a1c2e70f82bf022ed0a6efcbb\" class=\"spinner-border text-info\" role=\"status\">\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "            .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            .p-Widget.jp-RenderedJavaScript.jp-mod-trusted.jp-OutputArea-output:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            </style>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdn.datatables.net/1.10.20/css/jquery.dataTables.min.css\">\n",
       "            <table id=\"table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\" class=\"display\" style=\"display:block\"></table>\n",
       "            <script>\n",
       "              \n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\")) {\n",
       "              dt = $(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\").dataTable();\n",
       "            } else if ($(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'vrts.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: \"2019.01.25\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Day\\n|\\nDownloaded: True\\nCopied: True\", 0: 0}, {1: \"2019.02.02\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Night\\n|\\nDownloaded: True\\nCopied: True\", 0: 1}, {1: \"2020.01.25\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Day\\n|\\nDownloaded: True\\nCopied: True\", 0: 2}, {1: \"2020.02.02\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Night\\n|\\nDownloaded: True\\nCopied: True\", 0: 3}])\n",
       "              .draw('full-hold');\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\")) {\n",
       "              dt = $(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\").dataTable();\n",
       "            } else if ($(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'vrts.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: \"2019.01.25\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Day\\n|\\nDownloaded: True\\nCopied: True\", 0: 0}, {1: \"2019.02.02\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Night\\n|\\nDownloaded: True\\nCopied: True\", 0: 1}, {1: \"2020.01.25\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Day\\n|\\nDownloaded: True\\nCopied: True\", 0: 2}, {1: \"2020.02.02\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Night\\n|\\nDownloaded: True\\nCopied: True\", 0: 3}])\n",
       "              .draw('full-hold');\n",
       "          });\n",
       "        }\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            $(\"#progress_indicator_83d15d4a1c2e70f82bf022ed0a6efcbb\").remove();\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            $(\"#progress_indicator_83d15d4a1c2e70f82bf022ed0a6efcbb\").remove();\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\")) {\n",
       "              dt = $(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\").dataTable();\n",
       "            } else if ($(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'vrts.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: \"2019.01.25\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Day\\n|\\nDownloaded: True\\nCopied: True\", 0: 0}, {1: \"2019.02.02\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Night\\n|\\nDownloaded: True\\nCopied: True\", 0: 1}, {1: \"2020.01.25\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Day\\n|\\nDownloaded: True\\nCopied: True\", 0: 2}, {1: \"2020.02.02\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Night\\n|\\nDownloaded: True\\nCopied: True\", 0: 3}])\n",
       "              .draw('full-hold');\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\")) {\n",
       "              dt = $(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\").dataTable();\n",
       "            } else if ($(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_8dd8fa2eb0734b5dc9433049ba9e36e2\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'vrts.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: \"2019.01.25\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Day\\n|\\nDownloaded: True\\nCopied: True\", 0: 0}, {1: \"2019.02.02\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Night\\n|\\nDownloaded: True\\nCopied: True\", 0: 1}, {1: \"2020.01.25\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Day\\n|\\nDownloaded: True\\nCopied: True\", 0: 2}, {1: \"2020.02.02\\n['h17v03', 'h18v03', 'h17v04', 'h18v04']\\nLST_Night\\n|\\nDownloaded: True\\nCopied: True\", 0: 3}])\n",
       "              .draw('full-hold');\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# need to wait here\n",
    "#vrts = beam.Pipeline(InteractiveRunner()) | beam.Create(DATES_TO_TEST) | CreateVrtsForDays(hdf_bucket_path)\n",
    "vrts = im_info_with_tile_paths | BuildVrt()\n",
    "ib.show(vrts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_tiffs = vrts | TranslateVrtToLstTiff() | CreateProjectedOutput() | beam.ParDo(UploadAndClean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div id=\"progress_indicator_5471b028b81c2562da6949bf3422769e\" class=\"spinner-border text-info\" role=\"status\">\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "            .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            .p-Widget.jp-RenderedJavaScript.jp-mod-trusted.jp-OutputArea-output:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            </style>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdn.datatables.net/1.10.20/css/jquery.dataTables.min.css\">\n",
       "            <table id=\"table_df_d4a5965e9b1934d2345c05a3cf47191d\" class=\"display\" style=\"display:block\"></table>\n",
       "            <script>\n",
       "              \n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\")) {\n",
       "              dt = $(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\").dataTable();\n",
       "            } else if ($(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'uploaded_tiffs.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2019.0125.Data.1km.Data.tif', 0: 0}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2019.0125.Data.1km.Data.tif', 0: 1}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2019.0202.Data.1km.Data.tif', 0: 2}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2019.0202.Data.1km.Data.tif', 0: 3}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2020.0125.Data.1km.Data.tif', 0: 4}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2020.0125.Data.1km.Data.tif', 0: 5}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2020.0202.Data.1km.Data.tif', 0: 6}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2020.0202.Data.1km.Data.tif', 0: 7}])\n",
       "              .draw('full-hold');\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\")) {\n",
       "              dt = $(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\").dataTable();\n",
       "            } else if ($(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'uploaded_tiffs.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2019.0125.Data.1km.Data.tif', 0: 0}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2019.0125.Data.1km.Data.tif', 0: 1}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2019.0202.Data.1km.Data.tif', 0: 2}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2019.0202.Data.1km.Data.tif', 0: 3}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2020.0125.Data.1km.Data.tif', 0: 4}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2020.0125.Data.1km.Data.tif', 0: 5}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2020.0202.Data.1km.Data.tif', 0: 6}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2020.0202.Data.1km.Data.tif', 0: 7}])\n",
       "              .draw('full-hold');\n",
       "          });\n",
       "        }\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            $(\"#progress_indicator_5471b028b81c2562da6949bf3422769e\").remove();\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            $(\"#progress_indicator_5471b028b81c2562da6949bf3422769e\").remove();\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\")) {\n",
       "              dt = $(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\").dataTable();\n",
       "            } else if ($(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'uploaded_tiffs.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2019.0125.Data.1km.Data.tif', 0: 0}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2019.0125.Data.1km.Data.tif', 0: 1}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2019.0202.Data.1km.Data.tif', 0: 2}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2019.0202.Data.1km.Data.tif', 0: 3}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2020.0125.Data.1km.Data.tif', 0: 4}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2020.0125.Data.1km.Data.tif', 0: 5}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2020.0202.Data.1km.Data.tif', 0: 6}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2020.0202.Data.1km.Data.tif', 0: 7}])\n",
       "              .draw('full-hold');\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            var dt;\n",
       "            if ($.fn.dataTable.isDataTable(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\")) {\n",
       "              dt = $(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\").dataTable();\n",
       "            } else if ($(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d_wrapper\").length == 0) {\n",
       "              dt = $(\"#table_df_d4a5965e9b1934d2345c05a3cf47191d\").dataTable({\n",
       "                \n",
       "            bAutoWidth: false,\n",
       "            columns: [{'title': ''}, {'title': 'uploaded_tiffs.0'}],\n",
       "            destroy: true,\n",
       "            responsive: true,\n",
       "            columnDefs: [\n",
       "              {\n",
       "                targets: \"_all\",\n",
       "                className: \"dt-left\"\n",
       "              },\n",
       "              {\n",
       "                \"targets\": 0,\n",
       "                \"width\": \"10px\",\n",
       "                \"title\": \"\"\n",
       "              }\n",
       "            ]\n",
       "              });\n",
       "            } else {\n",
       "              return;\n",
       "            }\n",
       "            dt.api()\n",
       "              .clear()\n",
       "              .rows.add([{1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2019.0125.Data.1km.Data.tif', 0: 0}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2019.0125.Data.1km.Data.tif', 0: 1}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2019.0202.Data.1km.Data.tif', 0: 2}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2019.0202.Data.1km.Data.tif', 0: 3}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2020.0125.Data.1km.Data.tif', 0: 4}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2020.0125.Data.1km.Data.tif', 0: 5}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_day/LST_Day_Unfilled.2020.0202.Data.1km.Data.tif', 0: 6}, {1: 'gs://hsg-dataflow-test/lst_download_dev/output-notebook/lst_night/LST_Night_Unfilled.2020.0202.Data.1km.Data.tif', 0: 7}])\n",
       "              .draw('full-hold');\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ib.show(uploaded_tiffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting it to run on dataflow\n",
    "\n",
    "The download part is easier, gdal isn't needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions, SetupOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions(flags=[])\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Sets the Google Cloud Region in which Cloud Dataflow will run.\n",
    "options.view_as(GoogleCloudOptions).region = 'europe-west4'\n",
    "\n",
    "# Because this notebook comes with a locally built version of the Beam Python SDK, we will need to set\n",
    "# the sdk_location option for the Dataflow Runner. You will not need to do this if you are using an\n",
    "# officially released version of Apache Beam.\n",
    "options.view_as(pipeline_options.SetupOptions).sdk_location = (\n",
    "    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' % \n",
    "    beam.version.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_gcs_location = BUCKET_PATH + \"/dataflow\"\n",
    "# Dataflow Staging Location.\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "\n",
    "# Dataflow Temp Location.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_HDF_BUCKET_PATH = BUCKET_PATH + '/hdf_df' #NB not os.path.join as that breaks on windows with backslash paths\n",
    "DF_OUTPUT_BUCKET_PATH = BUCKET_PATH + \"/output_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "p  = beam.Pipeline(DataflowRunner(), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    }
   ],
   "source": [
    "date_page_urls = (p | 'dates_to_process' >> beam.Create(DATES_TO_TEST)\n",
    "                   | 'date_page_urls' >> beam.ParDo(GetDatePageUrl(product_url)))\n",
    "hdf_urls = date_page_urls | GetHdfUrlsForDate()\n",
    "#ib.show(hdf_urls)\n",
    "\n",
    "required_for_download = hdf_urls | check_existing_files(DF_HDF_BUCKET_PATH, TILES_TO_TEST)\n",
    "\n",
    "downloaded_files = required_for_download | DownloadHdfsToBucket(username, password, DF_HDF_BUCKET_PATH)\n",
    "\n",
    "download_result = p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Click <a href=\"https://console.cloud.google.com/dataflow/jobs/europe-west4/2020-06-12_07_22_45-348952328589170505?project=map-oxford-hsg\" target=\"_new\">here</a> for the details of your Dataflow job!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' % \n",
    "      (download_result._job.location, download_result._job.id, download_result._job.projectId))\n",
    "display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still need to fix some import locations (import everything within the function where it's used). Not going to take this further in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### large functions below here to keep notebook a bit tidier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to run a given calculation against a GDAL raster\n",
    "\n",
    "This is based loosely on gdal_calc.py, it runs in blocks and saves to a sparse output raster. It uses numexpr to allow multithreaded computation of the actual calculation step and it also does multithreaded writing of the output. \n",
    "\n",
    "The calculation should be provided as a string with the input data being called 'band_data' e.g. for MODIS LST use \"band_data * 0.02 + (-273.15)\"\n",
    "e.g. `run_singleband_calculation(\"/tmp/2020.02.02/LST_Day.2020.02.02.vrt\", \"/tmp/2020.02.02/test_celsius_output.tif\", \"band_data * 0.02 + (-273.15)\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c2 = \"where(arr < 20, 20, (where (arr > 80, 80, arr)))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_singleband_calculation(input_singleband_file, out_file, calc, out_type='Float32'):\n",
    "    '''Hacked together from gdal_calc.py. Uses numexpr for slightly more efficient calculation (multithreaded).\n",
    "    \n",
    "    Calc must be the calculation to apply to the data from input_singleband_file, specified as a string which will be \n",
    "    eval'd against the data which will exist in a variable called band_data. i.e. to specify doubling the data then subtracting \n",
    "    three, provide calc=\"(band_data * 2.0) - 3.0\"'''\n",
    "    #input_datasets = []\n",
    "    #myBands = []\n",
    "    #myDataType = []\n",
    "    #myDataTypeNum = []\n",
    "    #myNDV = []\n",
    "    import numpy as np, numexpr as ne\n",
    "    DimensionsCheck = None\n",
    "    \n",
    "    ds = gdal.Open(input_singleband_file, gdal.GA_ReadOnly)\n",
    "    if not ds:\n",
    "        raise IOError(\"Error opening input file {}\".format(input_file))\n",
    "    input_dataset = ds\n",
    "    inputDataType = (gdal.GetDataTypeName(ds.GetRasterBand(1).DataType))\n",
    "    inputDataTypeNum = (ds.GetRasterBand(1).DataType)\n",
    "    inputNDV = (ds.GetRasterBand(1).GetNoDataValue())\n",
    "    \n",
    "    DimensionsCheck = [ds.RasterXSize, ds.RasterYSize]\n",
    "\n",
    "    if os.path.isfile(out_file):\n",
    "        os.remove(out_file)\n",
    "    # gdal_calc does this but it isn't valid as two int datasets can result in a float!\n",
    "    # outType = gdal.GetDataTypeName(max(myDataTypeNum))\n",
    "    \n",
    "    #create the output file\n",
    "    outDriver = gdal.GetDriverByName(\"GTiff\")\n",
    "    cOpts =  [\"TILED=YES\", \"SPARSE_OK=TRUE\", \"BLOCKXSIZE=1024\", \"BLOCKYSIZE=1024\", \"BIGTIFF=YES\", \"COMPRESS=LZW\", \"NUM_THREADS=ALL_CPUS\"]\n",
    "    outDS = outDriver.Create(out_file, DimensionsCheck[0], DimensionsCheck[1], 1, gdal.GetDataTypeByName(out_type), cOpts)\n",
    "    outDS.SetGeoTransform(input_dataset.GetGeoTransform())\n",
    "    outDS.SetProjection(input_dataset.GetProjection())\n",
    "    DefaultNDVLookup = {'Byte': 255, 'UInt16': 65535, 'Int16': -32767, 'UInt32': 4294967293, 'Int32': -2147483647, 'Float32': 3.402823466E+38, 'Float64': 1.7976931348623158E+308}\n",
    "    outBand = outDS.GetRasterBand(1)\n",
    "    outNDV = DefaultNDVLookup[out_type]\n",
    "    outBand.SetNoDataValue(outNDV)\n",
    "    outBand = None\n",
    "    \n",
    "    # vrt file reports a block size of 128*128 but the underlying hdf block size is 1200*100\n",
    "    # so hard code this or some clean multiple : this minimises disk access\n",
    "    myBlockSize = [4800,4800]\n",
    "    nXValid = myBlockSize[0]\n",
    "    nYValid = myBlockSize[1]\n",
    "    nXBlocks = (int)((DimensionsCheck[0] + myBlockSize[0] - 1) / myBlockSize[0]);\n",
    "    nYBlocks = (int)((DimensionsCheck[1] + myBlockSize[1] - 1) / myBlockSize[1]);\n",
    "    \n",
    "    for x in range(0, nXBlocks):\n",
    "        if x == nXBlocks-1:\n",
    "            nXValid = DimensionsCheck[0] - x * myBlockSize[0]\n",
    "        \n",
    "        myX = x * myBlockSize[0]\n",
    "        \n",
    "        nYValid = myBlockSize[1]\n",
    "        myBufSize = nXValid * nYValid\n",
    "        \n",
    "        for y in range(0, nYBlocks):\n",
    "            if y == nYBlocks-1:\n",
    "                nYValid = DimensionsCheck[1] - y * myBlockSize[1]\n",
    "                myBufSize = nXValid * nYValid\n",
    "                \n",
    "            myY = y * myBlockSize[1]\n",
    "            band_data = input_dataset.GetRasterBand(1).ReadAsArray(xoff=myX, yoff=myY, \n",
    "                                                                       win_xsize=nXValid, win_ysize=nYValid)\n",
    "            nodata_locs = band_data == inputNDV\n",
    "            \n",
    "            try:\n",
    "                result = ne.evaluate(calc)\n",
    "            except:\n",
    "                raise\n",
    "            \n",
    "            # apply ndv (set nodata cells to zero then add nodata value to these cells)\n",
    "            result = ((1 * (nodata_locs==0))*result + (outNDV * nodata_locs))\n",
    "            \n",
    "            outBand = outDS.GetRasterBand(1)\n",
    "            outBand.WriteArray(result, xoff=myX, yoff=myY)\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of calculation functions:\n",
    "\n",
    "Both produce a single band output. All input files are single band (or if not then only the first band will be used). The names refer to how many single band input files can be provided. If you have multiband rasters to work on, then first make singleband files from them i.e. .vrt files that select only the relevant band.\n",
    "\n",
    "Single band: \n",
    "\n",
    "`run_singleband_calculation(\"/tmp/2020.02.02/LST_Night.2020.02.02.vrt\", \"/tmp/2020.02.02/test_celsius_night.tif\", \"band_data * 0.02 + (-273.15)\")`\n",
    "\n",
    "Multi band:\n",
    "\n",
    "`\n",
    "run_multiband_calculation([\"/tmp/2020.02.02/LST_Day.2020.02.02.vrt\", \"/tmp/2020.02.02/LST_Night.2020.02.02.vrt\"], \n",
    "                          [\"day\", \"night\"], \n",
    "                          \"/tmp/2020.02.02/test_celsius_diurnaldiff.tif\", \n",
    "                          \"(day * 0.02 + (-273.15)) - (night * 0.02 + (-273.15))\")\n",
    "                          `\n",
    "                          \n",
    "single band function is strictly redundant but just a bit easier to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.23.0.dev0 for Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
